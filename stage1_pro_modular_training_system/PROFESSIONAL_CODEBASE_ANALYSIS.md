# üìä PROFESSIONAL CODEBASE ANALYSIS
## Stage-1 Pro Modular Training System - Complete File-by-File Review

---

## üéØ EXECUTIVE SUMMARY

**Total Files Analyzed**: 54 Python files across 10 directories
**Analysis Method**: Complete code inspection + import analysis + architecture review
**Date**: December 27, 2025
**Standard**: 2025 best practices for production-grade ML systems

---

## üìã OVERALL ASSESSMENT

| Category | Score | Notes |
|---------|-------|-------|
| **Architecture** | 7/10 | Good separation of concerns, but missing wrapper |
| **Code Quality** | 6/10 | Good comments/docs, some duplicates, partial implementations |
| **Phase Completeness** | 8/10 | Phases 1-3 complete, Phase 4 partial, Phase 5-6 stub |
| **Testing** | 5/10 | Acceptance tests exist, no local smoke tests |
| **Production Readiness** | 3/10 | Cannot run end-to-end, missing wrapper, missing smoke tests |

**Overall Assessment**: ‚ö†Ô∏è **60% Production Ready** - Needs wrapper + smoke tests + clean duplicates

---

## üîç DETAILED FILE ANALYSIS

### ‚úÖ **DIRECTORIES (10 TOTAL)**

#### 1. `model/` (6 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `backbone.py` | 147 | ‚úÖ GOOD | DINOv3Backbone wrapper, HuggingFace Transformers, correct API |
| `head.py` | 101 | ‚úÖ GOOD | Stage1Head with phase support, correct architecture |
| `gate_head.py` | 331 | ‚úÖ GOOD | GateHead with 3 heads, soft gates, correct SelectiveNet |
| `peft.py` | 292 | üóëÔ∏è DUPLICATE | OLD HuggingFace wrapper - DELETE (peft_integration.py is current) |
| `peft_custom.py` | 399 | üóëÔ∏è DUPLICATE | Custom DoRA fallback - DELETE (peft_integration.py is current) |
| `peft_integration.py` | 567 | ‚úÖ GOOD | "REAL" HuggingFace PEFT integration, production-ready |

**Issues Found**:
- **2 duplicate PEFT files** - DELETE `peft.py` and `peft_custom.py`, keep only `peft_integration.py`
- **No multi-view inference** - backbone.py only does single CLS token extraction
- **Missing MIL aggregation** - no max/top-K pooling logic

---

#### 2. `data/` (4 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `datasets.py` | 79 | ‚úÖ GOOD | NATIXDataset + MultiRoadworkDataset, timm augmentation, correct normalization |
| `loaders.py` | 189 | ‚úÖ GOOD | Dynamic batch size, OOM handling, HuggingFace support |
| `splits.py` | 121 | ‚úÖ EXCELLENT | 4-way deterministic splits, hash-based, NO leakage |
| `transforms.py` | 74 | ‚úÖ GOOD | Timm-style augmentation, aggressive/moderate modes |

**Issues Found**: None - data module is solid

---

#### 3. `training/` (7 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `trainer.py` | 700 | ‚úÖ GOOD | Stage1ProTrainer class, EMA, checkpointing, validation |
| `peft_real_trainer.py` | 567 | ‚úÖ GOOD | RealPEFTTrainer, adapter-only checkpoints, merge for inference |
| `optimizers.py` | 89 | ‚úÖ GOOD | AdamW optimizer with config |
| `schedulers.py` | 45 | ‚úÖ GOOD | CosineAnnealingLR scheduler |
| `ema.py` | 77 | ‚úÖ GOOD | EMA implementation, shadow weights |
| `losses.py` | 173 | ‚úÖ GOOD | SelectiveNet loss, auxiliary loss |
| `risk_training.py` | 65 | ‚ö†Ô∏è STUB | ConformalRiskTrainer - NOT IMPLEMENTED (Phase 6) |

**Issues Found**:
- **`risk_training.py` is stub** - raises NotImplementedError, needs Phase 6 implementation
- **No wrapper trainer** - scripts call Stage1ProTrainer directly without component creation

---

#### 4. `metrics/` (4 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `selective.py` | 123 | ‚úÖ GOOD | Risk-coverage, AUGRC, bootstrap CIs, selective metrics |
| `calibration.py` | 150+ | ‚úÖ GOOD | NLL, Brier, ECE, calibration metrics |
| `bootstrap.py` | ~100 | ‚úÖ GOOD | Bootstrap CI computation |
| `exit.py` | ~100 | ‚úÖ GOOD | Exit metrics |

**Issues Found**: None - metrics module is solid

---

#### 5. `utils/` (8 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `logging.py` | 232 | ‚úÖ GOOD | CSVLogger with selective metrics |
| `checkpointing.py` | 379 | ‚úÖ GOOD | Comprehensive checkpoint save/load/validation |
| `reproducibility.py` | 150 | ‚úÖ EXCELLENT | Seed setting, TF32, deterministic mode |
| `feature_cache.py` | 79 | ‚úÖ GOOD | Feature caching for fast iteration |
| `visualization.py` | 146 | ‚úÖ GOOD | Risk-coverage curves, AUGRC distribution |
| `json_schema.py` | 83 | ‚úÖ GOOD | JSON schema validation |
| `monitoring.py` | 60 | ‚úÖ GOOD | ProgressMonitor, GPUMonitor |
| `feature_cache.py` | 79 | ‚úÖ GOOD | Feature extraction caching |

**Issues Found**: None - utils module is solid

---

#### 6. `scripts/` (12 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `00_make_splits.py` | 79 | ‚úÖ GOOD | Creates 4-way splits, hash-based, correct |
| `20_train.py` | 111 | ‚ùå BROKEN | WRONG Stage1ProTrainer call - missing component creation |
| `25_threshold_sweep.py` | 257 | ‚úÖ GOOD | Threshold sweep on val_calib, FNR constraint |
| `33_calibrate_gate.py` | 471 | ‚úÖ GOOD | Gate calibration, BOTH constraints, Platt scaling |
| `40_eval_selective.py` | 576 | ‚úÖ GOOD | Selective eval on val_test, risk-coverage, AUGRC |
| `41_infer_gate.py` | 286 | ‚úÖ GOOD | Gate exit inference, Stage A/B cascade |
| `43_ab_test_peft.py` | 268 | ‚úÖ GOOD | A/B testing framework (full vs LoRA vs DoRA) |
| `44_explora_pretrain.py` | 314 | ‚úÖ GOOD | ExPLoRA pretraining (MAE objective) |
| `45_train_supervised_explora.py` | 340 | ‚úÖ GOOD | Supervised training with ExPLoRA backbone |
| `50_export_bundle.py` | 340 | ‚úÖ GOOD | Bundle export, mutual exclusivity validation |
| `visualize.py` | 252 | ‚úÖ GOOD | Risk-coverage plots, AUGRC distribution |
| `calibrate_gate.py` | 434 | üóëÔ∏è DUPLICATE | OLD gate calibration - DELETE (33_calibrate_gate.py is current) |

**Issues Found**:
- **`20_train.py` is BROKEN** - line 95: `trainer = Stage1ProTrainer(config, device=device, phase=args.phase)`
- **Wrong because**: `Stage1ProTrainer.__init__()` requires `model, backbone, train_loader, val_select_loader, val_calib_loader, config`
- **`calibrate_gate.py` is DUPLICATE** - DELETE, use `33_calibrate_gate.py`

---

#### 7. `calibration/` (4 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `gate_calib.py` | 300+ | ‚úÖ GOOD | Gate calibration logic |
| `scrc.py` | 84 | ‚ùå STUB | SCRC implementation - raises NotImplementedError |
| `dirichlet.py` | 200+ | ‚úÖ GOOD | Dirichlet calibration |
| `__init__.py` | ~20 | ‚úÖ GOOD | Module initialization |

**Issues Found**:
- **`scrc.py` is STUB** - Phase 6 implementation missing (lines 56, 80 raise NotImplementedError)

---

#### 8. `domain_adaptation/` (2 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `explora.py` | 300+ | ‚úÖ GOOD | ExPLoRA implementation |
| `data.py` | 100+ | ‚úÖ GOOD | Unlabeled dataset for ExPLoRA |

**Issues Found**: None - domain adaptation is solid

---

#### 9. `tests/` (2 files analyzed)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `test_peft_47_acceptance.py` | 332 | ‚úÖ GOOD | PEFT acceptance tests (r=1, adapter reload, A/B results) |
| `test_peft_47_simple.py` | ~200 | ‚úÖ GOOD | Simple PEFT unit tests |

**Issues Found**: None - PEFT tests are good

---

#### 10. Root Files (3 files)
| File | Lines | Status | Assessment |
|------|-------|--------|-----------|
| `config.py` | 368 | ‚úÖ GOOD | Stage1ProConfig dataclass, all hyperparameters |
| `cli.py` | 146 | ‚ùå BROKEN | Wrong Stage1ProTrainer call (same issue as 20_train.py) |
| `__init__.py` | 275 | ‚úÖ GOOD | Package initialization |

**Issues Found**:
- **`cli.py` is BROKEN** - line 132: same wrong Stage1ProTrainer call

---

## üö® CRITICAL ISSUES (Must Fix)

### 1. **NO WRAPPER ENTRYPOINT** ‚ùå CRITICAL

**Problem**: 
- Scattered scripts call `Stage1ProTrainer` directly without creating components
- `scripts/20_train.py` line 95: `trainer = Stage1ProTrainer(config, device=device, phase=args.phase)`
- `cli.py` line 132: Same wrong call

**Why this is wrong**:
```python
# Stage1ProTrainer.__init__() signature:
def __init__(
    self,
    model: nn.Module,           # ‚Üê REQUIRED
    backbone: nn.Module,       # ‚Üê REQUIRED
    train_loader,               # ‚Üê REQUIRED
    val_select_loader,           # ‚Üê REQUIRED
    val_calib_loader,           # ‚Üê REQUIRED
    config,
    device: str = "cuda",
    verbose: bool = True
):
```

**What wrapper should do**:
```python
def main():
    # 1. Load config
    config = Stage1ProConfig.load(args.config) or Stage1ProConfig()
    
    # 2. Create splits if needed
    if not os.path.exists(config.output_dir + "/splits.json"):
        from data.splits import create_val_splits
        create_val_splits(...)
    
    # 3. Load splits
    from data.splits import load_splits
    splits = load_splits("splits.json")
    
    # 4. Create dataloaders
    from data.loaders import create_data_loaders
    train_dataset = NATIXDataset(...)
    train_loader, val_select_loader, val_calib_loader = create_data_loaders(
        train_dataset, splits, config
    )
    
    # 5. Load backbone
    from model.backbone import DINOv3Backbone
    backbone = DINOv3Backbone(config.model_path)
    backbone.load(freeze=(config.phase == 1))
    
    # 6. Create head
    from model.head import Stage1Head
    head = Stage1Head(
        num_classes=config.num_classes,
        hidden_size=768,
        phase=config.phase
    )
    
    # 7. Create trainer with ALL components
    from training.trainer import Stage1ProTrainer
    trainer = Stage1ProTrainer(
        model=head,
        backbone=backbone,
        train_loader=train_loader,
        val_select_loader=val_select_loader,
        val_calib_loader=val_calib_loader,
        config=config,
        device=device
    )
    
    # 8. Train
    trainer.train()
```

**File to create**: `scripts/wrapper.py` - single official entrypoint

---

### 2. **NO LOCAL SMOKE TESTS** ‚ùå CRITICAL

**Problem**: 
- No documented way to run end-to-end pipeline locally
- Cannot verify phases 1-3 work without running on rented GPU
- No "smoke test" scripts to prove artifacts are created

**What smoke tests should do**:

#### Smoke Test A (Phase 1):
```bash
# Run 1 epoch, small batch, local 8GB GPU
python scripts/wrapper.py \
  --phase 1 \
  --exit_policy softmax \
  --epochs 1 \
  --max_batch_size 4 \
  --output_dir outputs/smoke_phase1

# Verify outputs exist:
# - outputs/smoke_phase1/model_best.pth
# - outputs/smoke_phase1/val_calib_logits.pt
# - outputs/smoke_phase1/val_calib_labels.pt
# - outputs/smoke_phase1/thresholds.json
# - outputs/smoke_phase1/bundle.json
```

#### Smoke Test B (Phase 3):
```bash
# Run 1 epoch, small batch, local 8GB GPU
python scripts/wrapper.py \
  --phase 3 \
  --exit_policy gate \
  --epochs 1 \
  --max_batch_size 4 \
  --output_dir outputs/smoke_phase3

# Verify outputs exist:
# - outputs/smoke_phase3/model_best.pth
# - outputs/smoke_phase3/val_calib_logits.pt
# - outputs/smoke_phase3/val_calib_gate_logits.pt
# - outputs/smoke_phase3/val_calib_labels.pt
# - outputs/smoke_phase3/gateparams.json
# - outputs/smoke_phase3/bundle.json
```

**File to create**: `scripts/smoke_test.py` - documented smoke test procedure

---

### 3. **MULTI-VIEW INFERENCE MISSING** ‚ö†Ô∏è HIGH PRIORITY

**Problem**: 
- `model/backbone.py` line 119: Only extracts CLS token from single image
- No tiling logic (3√ó3 = 9 crops)
- No MIL aggregation (max pooling or top-K mean)
- No batching of views

**What's needed** (from max.md):
- 1 global view (full image resized to 224)
- 3√ó3 tiles (9 crops) with 10-15% overlap
- Total: 10 views per image
- Batch all views together (single forward pass for speed)
- Aggregate with either:
  - Option A: `p = max_k p_k` (pure MIL - best recall)
  - Option B: `p = mean(topK(p_k, K=2))` (better precision)

**File to create**: `model/multi_view.py`

---

### 4. **DUPLICATE FILES** üóëÔ∏è CLEANUP NEEDED

#### Delete These Files:
1. `model/peft.py` (292 lines)
   - Reason: OLD HuggingFace wrapper, replaced by `peft_integration.py`
   
2. `model/peft_custom.py` (399 lines)
   - Reason: Custom DoRA fallback, replaced by `peft_integration.py`
   
3. `scripts/calibrate_gate.py` (434 lines)
   - Reason: OLD gate calibration, replaced by `33_calibrate_gate.py`

**Command to clean**:
```bash
cd /home/sina/projects/miner_b/stage1_pro_modular_training_system
rm model/peft.py
rm model/peft_custom.py
rm scripts/calibrate_gate.py
```

**Keep**:
- `model/peft_integration.py` (current, working)
- `scripts/33_calibrate_gate.py` (current, working)

---

## üìã PHASE-BY-PHASE COMPLETENESS

### Phase 0: Data + Splits
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| Splits creation | `data/splits.py` | ‚úÖ COMPLETE | No |
| Datasets | `data/datasets.py` | ‚úÖ COMPLETE | No |
| Loaders | `data/loaders.py` | ‚úÖ COMPLETE | No |
| Transforms | `data/transforms.py` | ‚úÖ COMPLETE | No |

**Phase 0 Status**: ‚úÖ **COMPLETE** (4/4 components)

---

### Phase 1: Baseline Training
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| DINOv3 backbone | `model/backbone.py` | ‚úÖ COMPLETE | No |
| Stage1 head | `model/head.py` | ‚úÖ COMPLETE | No |
| Trainer | `training/trainer.py` | ‚úÖ COMPLETE | No |
| Checkpointing | `utils/checkpointing.py` | ‚úÖ COMPLETE | No |
| Reproducibility | `utils/reproducibility.py` | ‚úÖ COMPLETE | No |
| OOM handling | `data/loaders.py` | ‚úÖ COMPLETE | No |
| Threshold sweep | `scripts/25_threshold_sweep.py` | ‚úÖ COMPLETE | No |
| Bundle export | `scripts/50_export_bundle.py` | ‚úÖ COMPLETE | No |
| **Wrapper entrypoint** | ‚ùå MISSING | YES |
| **Smoke test** | ‚ùå MISSING | YES |

**Phase 1 Status**: ‚ö†Ô∏è **8/10 COMPLETE** (missing wrapper + smoke test)

---

### Phase 2: Selective Metrics
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| Risk-coverage | `metrics/selective.py` | ‚úÖ COMPLETE | No |
| AUGRC | `metrics/selective.py` | ‚úÖ COMPLETE | No |
| Bootstrap CIs | `metrics/bootstrap.py` | ‚úÖ COMPLETE | No |
| Selective metrics | `metrics/selective.py` | ‚úÖ COMPLETE | No |
| Visualization | `scripts/visualize.py` | ‚úÖ COMPLETE | No |
| Eval script | `scripts/40_eval_selective.py` | ‚úÖ COMPLETE | No |

**Phase 2 Status**: ‚úÖ **COMPLETE** (6/6 components)

---

### Phase 3: Gate Head
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| Gate head | `model/gate_head.py` | ‚úÖ COMPLETE | No |
| SelectiveNet loss | `training/losses.py` | ‚úÖ COMPLETE | No |
| Gate calibration | `scripts/33_calibrate_gate.py` | ‚úÖ COMPLETE | No |
| Gate inference | `scripts/41_infer_gate.py` | ‚úÖ COMPLETE | No |
| **Smoke test** | ‚ùå MISSING | YES |

**Phase 3 Status**: ‚ö†Ô∏è **4/5 COMPLETE** (missing smoke test)

---

### Phase 4: Domain Adaptation + PEFT
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| PEFT integration | `model/peft_integration.py` | ‚úÖ COMPLETE | No |
| PEFT trainer | `training/peft_real_trainer.py` | ‚úÖ COMPLETE | No |
| A/B testing | `scripts/43_ab_test_peft.py` | ‚úÖ COMPLETE | No |
| ExPLoRA pretrain | `scripts/44_explora_pretrain.py` | ‚úÖ COMPLETE | No |
| ExPLoRA supervised | `scripts/45_train_supervised_explora.py` | ‚úÖ COMPLETE | No |
| **Acceptance test** | ‚ùå MISSING | YES |
| **Multi-view** | ‚ùå MISSING | YES |

**Phase 4 Status**: ‚ö†Ô∏è **5/7 COMPLETE** (missing acceptance test + multi-view)

---

### Phase 5: SCRC
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| SCRC calibrator | `calibration/scrc.py` | ‚ö†Ô∏è STUB | YES (NotImplementedError) |
| Risk trainer | `training/risk_training.py` | ‚ö†Ô∏è STUB | YES (NotImplementedError) |

**Phase 5 Status**: ‚è≠Ô∏è **0/2 COMPLETE** (both are stubs)

---

### Phase 6: Continuous Learning
| Component | File | Status | Missing? |
|----------|------|--------|----------|
| Continuous learning loop | ‚ùå MISSING | YES |

**Phase 6 Status**: ‚ùå **0/1 COMPLETE** (not started)

---

## üéØ COMPLETE ACTION PLAN (Priority Order)

### Phase 0: Cleanup (Day 1)

**Priority 0.1: Delete Duplicate Files** (30 minutes)
```bash
cd /home/sina/projects/miner_b/stage1_pro_modular_training_system

# Delete 3 duplicate files
rm model/peft.py
rm model/peft_custom.py
rm scripts/calibrate_gate.py

# Verify no duplicates remain
ls model/peft*.py  # Should show only peft_integration.py
ls scripts/*calibrate_gate*.py  # Should show only 33_calibrate_gate.py
```

**Priority 0.2: Fix Import Errors** (1 hour)
- Ensure all imports use consistent pattern
- Test imports from project root
- Verify `python -c "from model.backbone import DINOv3Backbone"` works
- Verify all module `__init__.py` files are correct

**Priority 0.3: Document Codebase** (2 hours)
- Create `ARCHITECTURE.md` explaining all components
- Create `DATA_FLOW.md` explaining how phases work
- Create `TESTING_PROTOCOL.md` explaining smoke tests

---

### Phase 1: Create Wrapper Entrypoint (Day 1-2)

**Priority 1.1: Create `scripts/wrapper.py`** (4-6 hours)

```python
"""
Official Wrapper Entrypoint - Phase 0 (Foundation)

Single entrypoint that:
1. Loads config
2. Creates/loaders all required components
3. Calls Stage1ProTrainer correctly
4. Supports all phases (1-6)
5. Enforces correct order of operations
"""

import argparse
import os
import sys
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import Stage1ProConfig
from data.datasets import NATIXDataset, MultiRoadworkDataset
from data.splits import create_val_splits, load_splits
from data.loaders import create_data_loaders
from model.backbone import DINOv3Backbone
from model.head import Stage1Head
from model.gate_head import GateHead
from training.trainer import Stage1ProTrainer


def create_backbone(config, device):
    """Create and load backbone"""
    backbone = DINOv3Backbone(config.model_path)
    backbone.load(freeze=(config.phase == 1))
    return backbone.to(device)


def create_head(config, device):
    """Create head based on phase"""
    if config.phase <= 1:
        head = Stage1Head(
            num_classes=config.num_classes,
            hidden_size=768,
            phase=config.phase
        ).to(device)
    else:
        # Phase 3+: Use gate head
        from model.gate_head import GateHead
        head = GateHead(
            backbone_dim=768,
            num_classes=config.num_classes,
            gate_hidden_dim=128
        ).to(device)
    return head


def create_loaders(config, splits):
    """Create all data loaders"""
    # Load datasets
    train_dataset = NATIXDataset(
        image_dir=config.train_image_dir,
        labels_file=config.train_labels_file,
        processor=None,  # Will load from backbone
        augment=True
    )
    
    val_dataset = NATIXDataset(
        image_dir=config.val_image_dir,
        labels_file=config.val_labels_file,
        processor=None,
        augment=False
    )
    
    # Create loaders
    train_loader, val_select_loader, val_calib_loader = create_data_loaders(
        train_dataset,
        val_dataset,
        splits,
        config
    )
    
    return train_loader, val_select_loader, val_calib_loader


def main():
    parser = argparse.ArgumentParser(
        description="Stage-1 Pro Modular Training System - Official Wrapper Entrypoint",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # Phase and mode
    parser.add_argument("--phase", type=int, choices=[1, 2, 3, 4, 5, 6], default=1)
    parser.add_argument("--exit_policy", type=str, choices=["softmax", "gate", "scrc"], default="softmax")
    parser.add_argument("--config", type=str, default="config.yaml")
    
    # Data paths
    parser.add_argument("--train_image_dir", type=str)
    parser.add_argument("--train_labels_file", type=str)
    parser.add_argument("--val_image_dir", type=str)
    parser.add_argument("--val_labels_file", type=str)
    
    # Training args
    parser.add_argument("--output_dir", type=str, default="outputs")
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--max_batch_size", type=int)
    parser.add_argument("--device", type=str, default="cuda")
    
    args = parser.parse_args()
    
    # Load config
    config = Stage1ProConfig.load(args.config) if os.path.exists(args.config) else Stage1ProConfig()
    
    # Update config with args
    for key, value in vars(args).items():
        if value is not None and hasattr(config, key):
            if key not in ['phase']:  # phase is read-only property
                setattr(config, key, value)
    
    # Device
    device = args.device if args.device == "cuda" and torch.cuda.is_available() else "cpu"
    
    # Create splits if needed
    splits_path = Path(args.output_dir) / "splits.json"
    if not splits_path.exists():
        print(f"Creating splits...")
        splits = create_val_splits(
            val_dataset,
            val_select_ratio=config.val_select_ratio,
            val_calib_ratio=config.val_calib_ratio,
            val_test_ratio=config.val_test_ratio,
            seed=42
        )
        from data.splits import save_splits
        save_splits(splits, str(splits_path))
    else:
        print(f"Loading splits from {splits_path}...")
        splits = load_splits(str(splits_path))
    
    # Create components
    print(f"Creating backbone...")
    backbone = create_backbone(config, device)
    
    print(f"Creating head...")
    head = create_head(config, device)
    
    print(f"Creating data loaders...")
    train_loader, val_select_loader, val_calib_loader = create_loaders(config, splits)
    
    # Create trainer
    print(f"Creating trainer...")
    trainer = Stage1ProTrainer(
        model=head,
        backbone=backbone,
        train_loader=train_loader,
        val_select_loader=val_select_loader,
        val_calib_loader=val_calib_loader,
        config=config,
        device=device
    )
    
    # Train
    print(f"Starting Phase {config.phase} training...")
    trainer.train()


if __name__ == "__main__":
    main()
```

**Priority 1.2: Create `scripts/smoke_test.py`** (2 hours)

```python
"""
Local Smoke Tests - Verify Pipeline End-to-End

Runs tiny 1-epoch training to verify:
1. Components create correctly
2. Training runs without errors
3. Required artifacts are created
4. Bundle validation passes
"""

import argparse
import os
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from config import Stage1ProConfig


def smoke_test_phase1():
    """Smoke test Phase 1 baseline"""
    print("\n" + "="*80)
    print("SMOKE TEST: Phase 1 (Baseline)")
    print("="*80 + "\n")
    
    # Create minimal config
    config = Stage1ProConfig()
    config.phase = 1
    config.exit_policy = "softmax"
    config.epochs = 1
    config.max_batch_size = 4
    config.output_dir = "outputs/smoke_phase1"
    config.train_image_dir = "data/natix_official/train"
    config.train_labels_file = "data/natix_official/train_labels.csv"
    config.val_image_dir = "data/natix_official/val"
    config.val_labels_file = "data/natix_official/val_labels.csv"
    
    # Save config
    from config import Stage1ProConfig
    config_path = Path("outputs/smoke_phase1/config.yaml")
    config_path.parent.mkdir(parents=True, exist_ok=True)
    config.save(str(config_path))
    
    # Run wrapper
    import subprocess
    result = subprocess.run([
        sys.executable,
        "scripts/wrapper.py",
        "--phase", "1",
        "--exit_policy", "softmax",
        "--config", str(config_path),
        "--epochs", "1",
        "--max_batch_size", "4",
        "--output_dir", "outputs/smoke_phase1"
    ], capture_output=True, text=True)
    
    print(result.stdout)
    if result.returncode != 0:
        print(f"\n‚ùå SMOKE TEST FAILED with exit code {result.returncode}")
        return False
    
    # Verify artifacts
    print("\n" + "-"*80)
    print("VERIFYING ARTIFACTS...")
    print("-"*80 + "\n")
    
    required_artifacts = [
        "model_best.pth",
        "val_calib_logits.pt",
        "val_calib_labels.pt",
        "thresholds.json",
        "bundle.json"
    ]
    
    all_exist = True
    for artifact in required_artifacts:
        artifact_path = Path("outputs/smoke_phase1") / artifact
        if artifact_path.exists():
            print(f"‚úÖ {artifact} exists")
        else:
            print(f"‚ùå {artifact} MISSING")
            all_exist = False
    
    print("\n" + "="*80)
    if all_exist:
        print("‚úÖ SMOKE TEST PASSED - All artifacts created")
    else:
        print("‚ùå SMOKE TEST FAILED - Missing artifacts")
    print("="*80 + "\n")
    
    return all_exist


def smoke_test_phase3():
    """Smoke test Phase 3 gate head"""
    print("\n" + "="*80)
    print("SMOKE TEST: Phase 3 (Gate Head)")
    print("="*80 + "\n")
    
    # Create minimal config
    config = Stage1ProConfig()
    config.phase = 3
    config.exit_policy = "gate"
    config.epochs = 1
    config.max_batch_size = 4
    config.output_dir = "outputs/smoke_phase3"
    config.train_image_dir = "data/natix_official/train"
    config.train_labels_file = "data/natix_official/train_labels.csv"
    config.val_image_dir = "data/natix_official/val"
    config.val_labels_file = "data/natix_official/val_labels.csv"
    
    # Save config
    config_path = Path("outputs/smoke_phase3/config.yaml")
    config_path.parent.mkdir(parents=True, exist_ok=True)
    config.save(str(config_path))
    
    # Run wrapper
    import subprocess
    result = subprocess.run([
        sys.executable,
        "scripts/wrapper.py",
        "--phase", "3",
        "--exit_policy", "gate",
        "--config", str(config_path),
        "--epochs", "1",
        "--max_batch_size", "4",
        "--output_dir", "outputs/smoke_phase3"
    ], capture_output=True, text=True)
    
    print(result.stdout)
    if result.returncode != 0:
        print(f"\n‚ùå SMOKE TEST FAILED with exit code {result.returncode}")
        return False
    
    # Verify artifacts
    print("\n" + "-"*80)
    print("VERIFYING ARTIFACTS...")
    print("-"*80 + "\n")
    
    required_artifacts = [
        "model_best.pth",
        "val_calib_logits.pt",
        "val_calib_gate_logits.pt",
        "val_calib_labels.pt",
        "gateparams.json",
        "bundle.json"
    ]
    
    all_exist = True
    for artifact in required_artifacts:
        artifact_path = Path("outputs/smoke_phase3") / artifact
        if artifact_path.exists():
            print(f"‚úÖ {artifact} exists")
        else:
            print(f"‚ùå {artifact} MISSING")
            all_exist = False
    
    print("\n" + "="*80)
    if all_exist:
        print("‚úÖ SMOKE TEST PASSED - All artifacts created")
    else:
        print("‚ùå SMOKE TEST FAILED - Missing artifacts")
    print("="*80 + "\n")
    
    return all_exist


def main():
    parser = argparse.ArgumentParser(description="Run local smoke tests")
    parser.add_argument("--phase", type=int, choices=[1, 3], default=1)
    args = parser.parse_args()
    
    if args.phase == 1:
        passed = smoke_test_phase1()
    elif args.phase == 3:
        passed = smoke_test_phase3()
    
    sys.exit(0 if passed else 1)


if __name__ == "__main__":
    main()
```

---

### Phase 2: Implement Multi-View Inference (Day 2-3)

**Priority 2.1: Create `model/multi_view.py`** (4-6 hours)

```python
"""
Multi-View Inference Module - Production-Grade Implementation

Implements:
- Global view generation
- 3√ó3 tile generation with overlap
- Batch all views together (single forward pass)
- MIL aggregation (max pooling or top-K mean)
- Optional test-time augmentation (TTA) with flip

Based on:
- ICCV 2021: Better Aggregation in Test-Time Augmentation
- NeurIPS 2024: Multi-view pooling strategies
"""

import torch
import torch.nn as nn
from typing import Tuple, Optional, Literal
from torchvision import transforms as T


class MultiViewInference(nn.Module):
    """
    Multi-view inference with MIL aggregation.
    
    Generates 1 global + 3√ó3 tiles (10 total views),
    batches them for efficiency, and aggregates using MIL strategies.
    """
    
    def __init__(
        self,
        backbone: nn.Module,
        head: nn.Module,
        tile_size: int = 224,
        overlap: float = 0.1,
        aggregation_method: Literal['max', 'topk'] = 'max',
        top_k: int = 2,
        use_tta: bool = False
    ):
        super().__init__()
        self.backbone = backbone
        self.head = head
        self.tile_size = tile_size
        self.overlap = overlap
        self.aggregation_method = aggregation_method
        self.top_k = top_k
        self.use_tta = use_tta
    
    def generate_views(self, image: torch.Tensor) -> torch.Tensor:
        """
        Generate views: 1 global + 3√ó3 tiles
        
        Args:
            image: Input image [3, H, W]
        
        Returns:
            views: [N_views, 3, H, W] where N_views = 10 (no TTA) or 20 (with TTA)
        """
        c, h, w = image.shape
        
        # Global view (resize to tile_size)
        global_view = T.Resize(self.tile_size)(image)
        
        # 3√ó3 tiles with overlap
        tile_h = int(h / 3 * (1 + self.overlap))
        tile_w = int(w / 3 * (1 + self.overlap))
        
        views = [global_view]
        
        for i in range(3):
            for j in range(3):
                y = int(i * tile_h)
                x = int(j * tile_w)
                
                # Crop tile
                tile = T.CenterCrop((tile_w, tile_h))(
                    image[:, y:y+tile_h, x:x+tile_w]
                )
                
                # Resize tile to tile_size
                tile_resized = T.Resize(self.tile_size)(tile)
                views.append(tile_resized)
                
                # Add TTA (horizontal flip) if enabled
                if self.use_tta:
                    flipped = T.RandomHorizontalFlip(p=1.0)(tile_resized)
                    views.append(flipped)
        
        # Stack: [N_views, 3, H, W]
        return torch.stack(views, dim=0)
    
    def aggregate(self, probs: torch.Tensor) -> torch.Tensor:
        """
        MIL aggregation across views
        
        Args:
            probs: View probabilities [B, N_views, 2]
        
        Returns:
            aggregated_probs: [B, 2]
        """
        if self.aggregation_method == 'max':
            # Max pooling (pure MIL) - best recall
            return probs.max(dim=1)
        
        elif self.aggregation_method == 'topk':
            # Top-K mean - better precision with almost same recall
            topk_probs, _ = probs.topk(self.top_k, dim=1)
            return topk_probs.mean(dim=1)
        
        else:
            raise ValueError(f"Unknown aggregation method: {self.aggregation_method}")
    
    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """
        Forward pass: generate views ‚Üí batch inference ‚Üí aggregate
        
        Args:
            image: Input image [3, H, W]
        
        Returns:
            final_probs: [B, 2]
        """
        # Generate views
        views = self.generate_views(image)  # [N_views, 3, H, W]
        
        # Batch all views together for efficiency
        # Reshape to [B * N_views, 3, H, W] for backbone
        batch_size = views.shape[0]
        n_views = views.shape[1]
        views_batched = views.view(batch_size * n_views, 3, self.tile_size, self.tile_size)
        
        # Extract features for all views
        with torch.no_grad():
            features = self.backbone.extract_features(views_batched)  # [B * N_views, hidden_dim]
        
        # Reshape back: [B, N_views, hidden_dim]
        features = features.view(batch_size, n_views, -1)
        
        # Get head outputs for all views
        logits = self.head(features)  # [B, N_views, 2]
        probs = torch.softmax(logits, dim=-1)  # [B, N_views, 2]
        
        # Aggregate
        final_probs = self.aggregate(probs)
        
        return final_probs


def test_multi_view():
    """Test multi-view inference with dummy data"""
    print("Testing Multi-View Inference...")
    
    # Create dummy backbone and head
    backbone = nn.Sequential(
        nn.Linear(3 * 224 * 224, 768),
        nn.ReLU(),
        nn.Linear(768, 768)
    )
    head = nn.Linear(768, 2)
    
    # Create multi-view inference
    mvi = MultiViewInference(
        backbone=backbone,
        head=head,
        tile_size=224,
        overlap=0.1,
        aggregation_method='max',
        top_k=2,
        use_tta=False
    )
    
    # Test with dummy image
    dummy_image = torch.randn(1, 3, 224, 224)
    probs = mvi(dummy_image)
    
    print(f"‚úÖ Multi-view inference working!")
    print(f"   Input shape: {dummy_image.shape}")
    print(f"   Output probs shape: {probs.shape}")
    print(f"   Output probs: {probs}")


if __name__ == "__main__":
    test_multi_view()
```

**Priority 2.2: Integrate Multi-View into Wrapper** (2 hours)
- Add `--use_multi_view` flag to wrapper
- Wrap model with MultiViewInference when enabled
- Default to False for backward compatibility

---

### Phase 3: Fix Stub Implementations (Day 3-4)

**Priority 3.1: Implement SCRC** (4-6 hours)

**File**: `calibration/scrc.py`

**Current issues** (lines 56, 80):
```python
def fit(self, ...):
    # TODO: Implement SCRC fitting (Phase 6)
    raise NotImplementedError("SCRC fitting - Phase 6 only")

def predict(self, ...):
    # TODO: Implement SCRC inference (Phase 6)
    raise NotImplementedError("SCRC inference - Phase 6 only")
```

**Implementation needed**:
```python
def fit(self, gate_scores, class_logits, labels, target_fnr=0.02, alpha=0.05):
    """
    Fit SCRC-I calibrator (calibration-only variant)
    
    Implements:
    1. Selection control: Œª1 threshold for gate acceptance
    2. Risk control: Œª2 threshold for set size control
    
    Args:
        gate_scores: Gate scores [N]
        class_logits: Class logits [N, 2]
        labels: Ground truth labels [N]
        target_fnr: Target FNR on exited samples
        alpha: Calibration alpha (confidence level)
    """
    import numpy as np
    from scipy.special import logit
    
    # Compute gate correctness
    class_probs = softmax(class_logits, axis=1)
    predicted_labels = np.argmax(class_probs, axis=1)
    is_correct = (predicted_labels == labels).astype(int)
    
    # Stage 1: Selection control (compute Œª1)
    # Use percentile to satisfy FNR constraint
    # For gate scores, we want high scores for correct samples
    # Œª1 is threshold: accept if gate_score >= Œª1
    correct_scores = gate_scores[is_correct == 1]
    self.lambda1 = np.percentile(correct_scores, 100 * (1 - target_fnr))
    
    # Stage 2: Risk control (compute Œª2)
    # Sort predictions by max probability
    max_probs = class_probs.max(axis=1)
    
    # Œª2 threshold for set size control
    # We want set size such that FNR ‚â§ target_fnr
    # Using percentile method
    self.lambda2 = np.percentile(max_probs, 100 * (1 - target_fnr))
    
    self.fitted = True
    
    print(f"‚úÖ SCRC fitted:")
    print(f"   Œª1 (selection threshold): {self.lambda1:.4f}")
    print(f"   Œª2 (risk threshold): {self.lambda2:.4f}")


def predict(self, gate_score, class_logits):
    """
    Predict prediction set: {0}, {1}, or {0,1}
    
    Args:
        gate_score: Gate score for this sample
        class_logits: Class logits [2]
    
    Returns:
        Prediction set: {0}, {1}, or {0,1}
    """
    if not self.fitted:
        raise ValueError("SCRC calibrator not fitted")
    
    import numpy as np
    
    # Get max probability for class logits
    class_probs = softmax(class_logits, axis=1)
    max_probs = class_probs.max(axis=1)
    predicted_class = np.argmax(class_probs, axis=1)
    
    # Decision:
    # If gate_score >= Œª1 AND max_prob >= Œª2: singleton {predicted_class}
    # Else: rejected set {0,1}
    
    if gate_score >= self.lambda1 and max_probs >= self.lambda2:
        return {predicted_class.item()}
    else:
        return {0, 1}
```

**Priority 3.2: Implement Conformal Risk Training** (4-6 hours)

**File**: `training/risk_training.py`

**Current issue** (line 59-64):
```python
def training_step(self, features, labels):
    # TODO: Implement conformal risk training step (Phase 6)
    raise NotImplementedError("ConformalRiskTrainer implementation - Phase 6 only")
```

**Implementation needed**:
```python
def training_step(self, features, labels):
    """
    Training step with batch splitting for conformal risk control
    
    Implements NeurIPS 2025 "End-to-End Optimization of Conformal Risk Control"
    
    Batch splitting: Each batch ‚Üí pseudo-calib (50%) + pseudo-pred (50%)
    Compute conformal threshold on calib set
    Backprop through risk-control objective on pred set
    
    Args:
        features: Input features [B, hidden_dim]
        labels: Ground truth labels [B]
    
    Returns:
        Total loss
    """
    batch_size = features.shape[0]
    split_idx = int(batch_size * self.batch_split_ratio)
    
    # Split batch
    calib_features = features[:split_idx]
    pred_features = features[split_idx:]
    calib_labels = labels[:split_idx]
    pred_labels = labels[split_idx:]
    
    # Get model predictions
    calib_outputs = self.model(calib_features)
    calib_probs = torch.softmax(calib_outputs, dim=1)
    calib_max_probs = calib_probs.max(dim=1)
    
    # Compute conformal threshold on calib set
    # Use lambda2 from SCRC calibrator (risk threshold)
    threshold = self.calibrator.lambda2
    
    # Apply threshold to prediction set
    pred_outputs = self.model(pred_features)
    pred_probs = torch.softmax(pred_outputs, dim=1)
    pred_max_probs = pred_probs.max(dim=1)
    
    # Compute conformal risk loss
    # Risk = Pr(wrong | accepted)
    # Accepted if max_prob >= threshold
    accepted_mask = pred_max_probs >= threshold
    
    if accepted_mask.sum() > 0:
        # Compute error on accepted samples
        pred_labels = torch.argmax(pred_outputs, dim=1)
        errors = (pred_labels[accepted_mask] != pred_labels[accepted_mask]).float().sum()
        risk = errors / accepted_mask.sum()
    else:
        risk = torch.tensor(1.0)  # Full risk if none accepted
    
    # Total loss
    loss = risk
    
    return loss
```

---

## üìä FINAL SUMMARY TABLE

| Issue | Priority | Files | Time | Status |
|-------|----------|-------|------|--------|
| Delete duplicates | P0 | 3 files | 30 min | ‚è≠Ô∏è PENDING |
| Create wrapper | P1 | `scripts/wrapper.py` | 4-6 hr | ‚è≠Ô∏è PENDING |
| Create smoke test | P1 | `scripts/smoke_test.py` | 2 hr | ‚è≠Ô∏è PENDING |
| Multi-view inference | P2 | `model/multi_view.py` | 4-6 hr | ‚è≠Ô∏è PENDING |
| Integrate multi-view | P2 | `scripts/wrapper.py` | 2 hr | ‚è≠Ô∏è PENDING |
| Implement SCRC | P3 | `calibration/scrc.py` | 4-6 hr | ‚è≠Ô∏è PENDING |
| Implement risk training | P3 | `training/risk_training.py` | 4-6 hr | ‚è≠Ô∏è PENDING |

**Total estimated time**: 20-30 hours

---

## üéØ ACCEPTANCE CRITERIA (Pass/Fail Gates)

### Gate Definition of "Done"
‚úÖ **Phase 1**: Wrapper + smoke test run end-to-end locally
‚úÖ **Phase 3**: Wrapper + smoke test run end-to-end locally
‚úÖ **Phase 4**: A/B test shows improvement on val_test (not just "code exists")
‚úÖ **Cleanup**: No duplicate files remain

### Fail Conditions
‚ùå **Wrapper missing**: Cannot claim "phase 1 done" without working entrypoint
‚ùå **Smoke test fails**: Cannot claim "phase 1 done" if artifacts not created locally
‚ùå **Duplicates remain**: Cannot claim "cleanup done" if 3 duplicates still exist

---

Generated: December 27, 2025
Status: Complete professional codebase analysis with action plan
Next Step: Waiting for your approval to start implementation

