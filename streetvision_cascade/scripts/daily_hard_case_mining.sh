#!/bin/bash
# ==============================================================================
# DAILY HARD-CASE MINING AUTOMATION
# Per REALISTIC_DEPLOYMENT_PLAN.md - December 20, 2025
#
# This script runs every day at 2 AM via cron:
# 0 2 * * * /path/to/streetvision_cascade/scripts/daily_hard_case_mining.sh
#
# Workflow:
# 1. Collect yesterday's validator queries (24 hours)
# 2. Run FiftyOne Brain hardness analysis
# 3. Extract top 200 hardest cases
# 4. Generate targeted SDXL synthetics (150 images)
# 5. Retrain DINOv3 classifier head (3 epochs)
# 6. Validate on challenge set
# 7. Deploy if improved (blue-green)
#
# Expected: +0.2-0.5% accuracy improvement per week
# ==============================================================================

set -e  # Exit on error

# Configuration
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
DATE_STR=$(date +%Y%m%d)
LOG_FILE="$PROJECT_DIR/logs/daily_mining_$DATE_STR.log"

# Create log directory
mkdir -p "$PROJECT_DIR/logs"

# Start logging
exec > >(tee -a "$LOG_FILE") 2>&1

echo "=============================================================="
echo "ðŸš€ DAILY HARD-CASE MINING STARTED"
echo "   Date: $(date)"
echo "   Project: $PROJECT_DIR"
echo "=============================================================="

# Activate Python environment (adjust as needed)
cd "$PROJECT_DIR"
if [ -d ".venv" ]; then
    source .venv/bin/activate
elif [ -d "venv" ]; then
    source venv/bin/activate
fi

# ==============================================================================
# STEP 1: Check model age (90-day retrain deadline)
# ==============================================================================
echo ""
echo "[1/7] Checking model age..."
python scripts/deployment/blue_green_deploy.py \
    --models-dir ./models \
    --config ./configs/cascade_config.yaml \
    --action check-age

# ==============================================================================
# STEP 2: Collect validator queries and run FiftyOne hard-case mining
# ==============================================================================
echo ""
echo "[2/7] Running FiftyOne hard-case mining..."
python scripts/active_learning/fiftyone_hard_mining.py \
    --queries-dir ./logs/validator_queries \
    --output-dir ./data/hard_cases \
    --hard-case-count 200 \
    --days 1

HARD_CASES_DIR="./data/hard_cases/batch_$DATE_STR"

# Check if hard cases were extracted
if [ ! -d "$HARD_CASES_DIR" ]; then
    echo "âš ï¸  No hard cases extracted. Skipping remaining steps."
    echo "   This is normal if no validator queries were received."
    exit 0
fi

HARD_CASE_COUNT=$(find "$HARD_CASES_DIR" -name "*.jpg" -o -name "*.png" 2>/dev/null | wc -l)
echo "   Hard cases extracted: $HARD_CASE_COUNT"

if [ "$HARD_CASE_COUNT" -lt 10 ]; then
    echo "âš ï¸  Too few hard cases ($HARD_CASE_COUNT). Skipping SDXL generation."
    SKIP_SDXL=true
fi

# ==============================================================================
# STEP 3: Generate targeted SDXL synthetics
# ==============================================================================
if [ "${SKIP_SDXL:-false}" != "true" ]; then
    echo ""
    echo "[3/7] Generating targeted SDXL synthetics..."
    
    # Generate 50 images for each of top 3 failure modes = 150 images
    python scripts/data/generate_sdxl_synthetic.py \
        --output-dir ./data/synthetic_sdxl/batch_$DATE_STR \
        --positive-count 75 \
        --negative-count 75 \
        --inference-steps 30 \
        --seed $(($(date +%s) % 100000))
        
    SYNTHETIC_COUNT=$(find "./data/synthetic_sdxl/batch_$DATE_STR" -name "*.png" 2>/dev/null | wc -l)
    echo "   Synthetics generated: $SYNTHETIC_COUNT"
else
    echo ""
    echo "[3/7] Skipping SDXL generation (too few hard cases)"
    SYNTHETIC_COUNT=0
fi

# ==============================================================================
# STEP 4: Prepare combined training dataset
# ==============================================================================
echo ""
echo "[4/7] Preparing combined training dataset..."

COMBINED_DIR="./data/combined_$DATE_STR"
mkdir -p "$COMBINED_DIR/positive" "$COMBINED_DIR/negative"

# Copy hard cases (they need labels from metadata)
echo "   Copying hard cases..."
for json_file in "$HARD_CASES_DIR"/*.json; do
    if [ -f "$json_file" ]; then
        # Read prediction from metadata
        prediction=$(python -c "import json; print(json.load(open('$json_file')).get('prediction', 0.5))")
        img_file="${json_file%.json}.jpg"
        if [ ! -f "$img_file" ]; then
            img_file="${json_file%.json}.png"
        fi
        
        if [ -f "$img_file" ]; then
            if (( $(echo "$prediction > 0.5" | bc -l) )); then
                cp "$img_file" "$COMBINED_DIR/positive/"
            else
                cp "$img_file" "$COMBINED_DIR/negative/"
            fi
        fi
    fi
done

# Copy synthetics
if [ -d "./data/synthetic_sdxl/batch_$DATE_STR" ]; then
    echo "   Copying synthetics..."
    cp "./data/synthetic_sdxl/batch_$DATE_STR/positive"/*.png "$COMBINED_DIR/positive/" 2>/dev/null || true
    cp "./data/synthetic_sdxl/batch_$DATE_STR/negative"/*.png "$COMBINED_DIR/negative/" 2>/dev/null || true
fi

# Link to base NATIX dataset if available
if [ -d "./data/natix_official/positive" ]; then
    echo "   Linking NATIX dataset..."
    for img in ./data/natix_official/positive/*; do
        ln -sf "$(realpath "$img")" "$COMBINED_DIR/positive/" 2>/dev/null || true
    done
    for img in ./data/natix_official/negative/*; do
        ln -sf "$(realpath "$img")" "$COMBINED_DIR/negative/" 2>/dev/null || true
    done
fi

POS_COUNT=$(find "$COMBINED_DIR/positive" -type f 2>/dev/null | wc -l)
NEG_COUNT=$(find "$COMBINED_DIR/negative" -type f 2>/dev/null | wc -l)
echo "   Combined dataset: $POS_COUNT positive, $NEG_COUNT negative"

# ==============================================================================
# STEP 5: Retrain DINOv3 classifier head
# ==============================================================================
echo ""
echo "[5/7] Retraining DINOv3 classifier head..."

# Only retrain if we have enough data
if [ $((POS_COUNT + NEG_COUNT)) -lt 50 ]; then
    echo "âš ï¸  Insufficient training data. Skipping retraining."
else
    python scripts/training/train_dinov3_classifier.py \
        --backbone-path ./models/stage1_dinov3/dinov2-large \
        --data-dir "$COMBINED_DIR" \
        --output-dir ./checkpoints/daily_$DATE_STR \
        --epochs 3 \
        --batch-size 32 \
        --lr 1e-4 \
        --use-focal-loss
        
    NEW_MODEL="./checkpoints/daily_$DATE_STR/dinov3_classifier_best.pth"
fi

# ==============================================================================
# STEP 6: Validate on challenge set
# ==============================================================================
echo ""
echo "[6/7] Validating on challenge set..."

if [ -f "$NEW_MODEL" ]; then
    # Get validation accuracy (placeholder - would run actual validation)
    echo "   New model: $NEW_MODEL"
    echo "   Running validation..."
    
    # In production: Run actual validation
    # python scripts/validation/evaluate.py --model $NEW_MODEL --challenge-set ./data/challenge_set
    
    # For now, proceed to deployment step
    DEPLOY_MODEL=true
else
    echo "   No new model to validate"
    DEPLOY_MODEL=false
fi

# ==============================================================================
# STEP 7: Deploy if improved (blue-green)
# ==============================================================================
echo ""
echo "[7/7] Deployment decision..."

if [ "${DEPLOY_MODEL:-false}" = "true" ] && [ -f "$NEW_MODEL" ]; then
    echo "   Deploying new model to GREEN environment..."
    
    python scripts/deployment/blue_green_deploy.py \
        --models-dir ./models \
        --config ./configs/cascade_config.yaml \
        --action deploy \
        --model-path "$NEW_MODEL" \
        --version "v2_daily_$DATE_STR"
        
    echo ""
    echo "   Running shadow traffic test..."
    python scripts/deployment/blue_green_deploy.py \
        --models-dir ./models \
        --config ./configs/cascade_config.yaml \
        --action test \
        --validation-set ./data/validation
        
    # In production: Would check test results and proceed with cutover
    # For now, just log
    echo ""
    echo "   âœ… Model deployed to GREEN. Run manual cutover when ready."
else
    echo "   No deployment needed today."
fi

# ==============================================================================
# SUMMARY
# ==============================================================================
echo ""
echo "=============================================================="
echo "âœ… DAILY HARD-CASE MINING COMPLETE"
echo "=============================================================="
echo "   Date: $(date)"
echo "   Hard cases mined: $HARD_CASE_COUNT"
echo "   Synthetics generated: $SYNTHETIC_COUNT"
echo "   Combined dataset: $((POS_COUNT + NEG_COUNT)) images"
echo ""
echo "   Logs: $LOG_FILE"
echo "=============================================================="

# Send Discord notification (if webhook configured)
if [ -n "$DISCORD_WEBHOOK" ]; then
    curl -s -X POST "$DISCORD_WEBHOOK" \
        -H "Content-Type: application/json" \
        -d "{\"content\":\"ðŸ“Š Daily mining complete: $HARD_CASE_COUNT hard cases, $SYNTHETIC_COUNT synthetics\"}" \
        > /dev/null
fi

echo "Done!"

