#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MAIN TRAINING SCRIPT - ULTIMATE DAYS 5-6
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Usage:
    python train_ultimate_day56.py --config configs/ultimate/training/pretrain_30ep.yaml

This is the MAIN training script for Days 5-6 implementation.

Pipeline:
1. Load NATIX dataset (8,549 train + 251 test)
2. Generate SAM 3 pseudo-labels (overnight, optional)
3. Compute GPS weights (BIGGEST WIN +7-10% MCC)
4. Create GPS-weighted sampler
5. Build complete 2026 model (DINOv3 + Qwen3-MoE + GAFM)
6. Train for 30 epochs with Sophia-H optimizer
7. Save best model (EMA weights)
8. Evaluate on validation set
9. Generate predictions for test set

Expected Results:
- Pre-training: MCC 0.94-0.96
- DoRA fine-tuning: MCC 0.96-0.97
- Final (ensemble + FOODS TTA): MCC 0.98-0.99 ğŸ†
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm
import wandb

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))

# Import project modules
from src.models.complete_model import CompleteRoadworkModel2026, ModelOutputs, create_model
from src.losses.combined_loss import CompleteLoss, create_loss
from src.data.dataset.natix_base import NATIXRoadworkDataset, create_dataloaders, NATIXRoadworkDataset as DatasetClass
from src.data.samplers.gps_weighted_sampler import GPSWeightedSampler, create_gps_sampler


def parse_args():
    """Parse command-line arguments"""
    parser = argparse.ArgumentParser(
        description="Train Ultimate 2026 Roadwork Detection Model (Days 5-6)",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Config
    parser.add_argument(
        '--config',
        type=str,
        default='configs/ultimate/training/pretrain_30ep.yaml',
        help='Path to training configuration YAML file'
    )
    
    # Model
    parser.add_argument(
        '--model-config',
        type=str,
        default='configs/ultimate/model/full_model.yaml',
        help='Path to model configuration YAML file'
    )
    
    # Loss
    parser.add_argument(
        '--loss-config',
        type=str,
        default='configs/ultimate/loss/combined_loss.yaml',
        help='Path to loss configuration YAML file'
    )
    
    # Output
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/checkpoints/pretrain',
        help='Directory to save checkpoints and logs'
    )
    
    # Options
    parser.add_argument(
        '--resume',
        type=str,
        default=None,
        help='Path to checkpoint to resume from'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Test setup without training'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable debug mode (slower but more info)'
    )
    
    # Device
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        choices=['cuda', 'cpu'],
        help='Device to train on'
    )
    
    # Seed
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility'
    )
    
    return parser.parse_args()


def load_configs(args):
    """Load all configuration files"""
    print("\n" + "="*60)
    print("ğŸ“‹ LOADING CONFIGURATION FILES")
    print("="*60)
    
    # Load training config
    with open(args.config, 'r') as f:
        train_config = yaml.safe_load(f)
    print(f"âœ… Training config: {args.config}")
    
    # Load model config
    with open(args.model_config, 'r') as f:
        model_config = yaml.safe_load(f)
    print(f"âœ… Model config: {args.model_config}")
    
    # Load loss config
    with open(args.loss_config, 'r') as f:
        loss_config = yaml.safe_load(f)
    print(f"âœ… Loss config: {args.loss_config}")
    
    # Merge configs
    config = {
        'training': train_config,
        'model': model_config,
        'loss': loss_config
    }
    
    print("="*60 + "\n")
    
    return config


def set_seed(seed: int):
    """Set random seed for reproducibility"""
    print(f"\nğŸ² Setting random seed to {seed}...")
    
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # Ensure deterministic behavior
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print("âœ… Random seed set\n")


def setup_device(device_name: str):
    """Setup device for training"""
    print(f"\nğŸ”§ Setting up device...")
    
    if device_name == 'cuda' and torch.cuda.is_available():
        device = torch.device('cuda')
        num_gpus = torch.cuda.device_count()
        print(f"âœ… Using CUDA with {num_gpus} GPU(s)")
        for i in range(num_gpus):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        device = torch.device('cpu')
        print("âš ï¸  Using CPU (CUDA not available or requested)")
    
    print(f"âœ… Device: {device}\n")
    
    return device


def initialize_wandb(config: dict, args):
    """Initialize Weights & Biases logging"""
    if not config['training'].get('logging', {}).get('enabled', False):
        print("\nâš ï¸  W&B logging disabled")
        return None
    
    wandb_config = config['training'].get('logging', {}).get('wandb', {})
    
    print(f"\nğŸ“Š Initializing W&B...")
    
    wandb.init(
        project=wandb_config.get('project_name', 'natix-roadwork-2026'),
        entity=wandb_config.get('entity', None),
        config={
            **config['training'],
            **config['model'],
            **config['loss']
        },
        mode=wandb_config.get('mode', 'online'),
        name=wandb_config.get('run_name', None),
        notes=wandb_config.get('notes', None),
        tags=wandb_config.get('tags', [])
    )
    
    print("âœ… W&B initialized\n")
    
    return wandb


def create_model_and_loss(config: dict, device: torch.device, args):
    """Create model and loss function"""
    print("\nğŸ§  Creating model and loss function...")
    
    # Create model
    print(f"   Loading model from: {args.model_config}")
    model = create_model(args.model_config)
    model = model.to(device)
    
    # Create loss
    print(f"   Loading loss from: {args.loss_config}")
    loss_fn = create_loss(args.loss_config)
    loss_fn = loss_fn.to(device) if hasattr(loss_fn, 'to') else loss_fn
    
    print("âœ… Model and loss function created\n")
    
    return model, loss_fn


def setup_dataloaders(config: dict, device: torch.device):
    """Setup train, val, and test dataloaders"""
    print("\nğŸ“¦ Setting up dataloaders...")
    
    # Import multi-view extractor (will be created later)
    from src.models.views.multi_view_extractor import MultiViewExtractor12
    
    # Create multi-view extractor
    multi_view_extractor = MultiViewExtractor12(config['model']['multi_view'])
    
    # Create dataloaders
    train_loader, val_loader, test_loader = create_dataloaders(
        config['data'],
        multi_view_extractor=multi_view_extractor
    )
    
    # Create GPS sampler (BIGGEST WIN!)
    if config['gps'].get('enabled', True):
        print(f"\nğŸ“ Creating GPS-weighted sampler (BIGGEST WIN +7-10% MCC)...")
        
        # Get test GPS coordinates
        test_dataset = test_loader.dataset
        test_gps = DatasetClass.get_gps_coordinates(test_dataset, split='test')
        
        # Create sampler
        train_dataset = train_loader.dataset
        sampler = create_gps_sampler(train_dataset, test_gps, config)
        
        # Apply sampler to train loader
        train_loader = DataLoader(
            train_dataset,
            batch_size=config['data']['batch_size'],
            sampler=sampler,
            num_workers=config['data'].get('num_workers', 4),
            pin_memory=True
        )
        
        print("âœ… GPS-weighted sampler created and applied\n")
    
    print("âœ… Dataloaders setup complete\n")
    
    return train_loader, val_loader, test_loader


def setup_optimizer(config: dict, model: nn.Module, device: torch.device):
    """Setup optimizer"""
    print("\nâš™ï¸  Setting up optimizer...")
    
    optimizer_config = config['model'].get('optimizer', {})
    
    # Try Sophia-H (2026 SOTA)
    try:
        from sophia import SophiaG
        print("   Using Sophia-H Optimizer (2Ã— faster convergence)")
        optimizer = SophiaG(
            model.parameters(),
            lr=config['training']['learning_rate'],
            betas=optimizer_config.get('sophia', {}).get('betas', [0.965, 0.99]),
            rho=optimizer_config.get('sophia', {}).get('rho', 0.04),
            weight_decay=config['training']['weight_decay']
        )
    except ImportError:
        print("âš ï¸  Sophia-H not available, using AdamW")
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config['training']['learning_rate'],
            betas=optimizer_config.get('adamw', {}).get('betas', [0.9, 0.999]),
            weight_decay=config['training']['weight_decay'],
            eps=optimizer_config.get('adamw', {}).get('eps', 1e-8)
        )
    
    print("âœ… Optimizer setup complete\n")
    
    return optimizer


def setup_scheduler(config: dict, optimizer, train_loader: DataLoader):
    """Setup learning rate scheduler"""
    print("\nğŸ“ˆ Setting up learning rate scheduler...")
    
    from transformers import get_cosine_schedule_with_warmup
    
    num_training_steps = len(train_loader) * config['training']['num_epochs']
    
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config['training'].get('warmup_steps', 500),
        num_training_steps=num_training_steps,
        num_cycles=0.5,  # Cosine decay
        min_lr_ratio=0.1  # Final LR = 10% of peak
    )
    
    print(f"âœ… Scheduler: Cosine with warmup ({config['training'].get('warmup_steps', 500)} steps)")
    print(f"   Total training steps: {num_training_steps}\n")
    
    return scheduler


def setup_ema(model: nn.Module, config: dict):
    """Setup Exponential Moving Average"""
    if not config['training'].get('ema', {}).get('enabled', True):
        print("\nâš ï¸  EMA disabled")
        return None
    
    print("\nğŸ“Š Setting up EMA (stability + 0.5-1% MCC)...")
    
    from src.utils.ema import EMA
    
    ema = EMA(
        model,
        decay=config['training'].get('ema', {}).get('decay', 0.9999)
    )
    
    print("âœ… EMA setup complete\n")
    
    return ema


def train_epoch(
    model: nn.Module,
    loss_fn: nn.Module,
    train_loader: DataLoader,
    optimizer,
    scheduler,
    ema,
    config: dict,
    device: torch.device,
    epoch: int,
    wandb_run
) -> float:
    """Train for one epoch"""
    
    model.train()
    
    total_loss = 0.0
    loss_dict_sum = {}
    batch_count = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{config['training']['num_epochs']}")
    
    for batch_idx, batch in enumerate(pbar):
        # Get data
        views = batch['image'].to(device)  # [B, 12, 3, 518, 518]
        labels = batch['label'].to(device)  # [B]
        metadata = {
            k: v.to(device) if torch.is_tensor(v) else v
            for k, v in batch['metadata'].items()
        }
        sam_masks = batch.get('sam3_masks', None)
        if sam_masks is not None:
            sam_masks = sam_masks.to(device)
        
        # Forward + Backward
        optimizer.zero_grad()
        
        # Get logits AND auxiliary outputs
        logits, aux_outputs = model(views, metadata, return_aux=True)
        
        # Calculate loss
        loss, curr_loss_dict = loss_fn(
            {
                'logits': logits,
                'view_features': aux_outputs.view_features,
                'aux_weather_logits': aux_outputs.aux_weather_logits,
                'seg_masks': aux_outputs.seg_masks
            },
            {
                'labels': labels,
                'weather_labels': metadata['weather'],
                'sam3_masks': sam_masks
            }
        )
        
        # Gradient accumulation
        loss = loss / config['training'].get('gradient_accumulation_steps', 1)
        
        # Backward
        loss.backward()
        
        # Gradient accumulation step
        if (batch_idx + 1) % config['training'].get('gradient_accumulation_steps', 1) == 0:
            # Clip gradients
            torch.nn.utils.clip_grad_norm_(
                model.parameters(),
                config['training'].get('max_grad_norm', 1.0)
            )
            
            # Step optimizer
            optimizer.step()
            optimizer.zero_grad()
            
            # Update EMA
            if ema is not None:
                ema.update(model)
            
            # Step scheduler
            if scheduler is not None:
                scheduler.step()
        
        # Accumulate losses
        total_loss += loss.item()
        for k, v in curr_loss_dict.items():
            if k not in loss_dict_sum:
                loss_dict_sum[k] = 0.0
            loss_dict_sum[k] += v
        
        batch_count += 1
        
        # Log to W&B
        if wandb_run is not None and batch_idx % 10 == 0:
            wandb_run.log({
                'train/loss': loss.item(),
                'train/lr': scheduler.get_last_lr()[0] if scheduler else config['training']['learning_rate'],
                **{f'train/{k}': v for k, v in loss_dict_sum.items()}
            })
        
        # Update progress bar
        pbar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'lr': f"{scheduler.get_last_lr()[0]:.2e}" if scheduler else f"{config['training']['learning_rate']:.2e}"
        })
    
    # Compute average loss
    avg_loss = total_loss / batch_count
    
    return avg_loss, loss_dict_sum


@torch.no_grad()
def evaluate(
    model: nn.Module,
    val_loader: DataLoader,
    device: torch.device
) -> float:
    """Evaluate on validation set"""
    
    model.eval()
    
    all_preds = []
    all_labels = []
    
    for batch in tqdm(val_loader, desc="Validating"):
        views = batch['image'].to(device)
        labels = batch['label'].to(device)
        metadata = {
            k: v.to(device) if torch.is_tensor(v) else v
            for k, v in batch['metadata'].items()
        }
        
        # Forward
        logits = model(views, metadata, return_aux=False)
        
        # Get predictions
        preds = logits.argmax(dim=-1)
        
        all_preds.append(preds.cpu())
        all_labels.append(labels.cpu())
    
    # Concatenate
    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)
    
    # Calculate MCC
    from sklearn.metrics import matthews_corrcoef
    mcc = matthews_corrcoef(all_labels.numpy(), all_preds.numpy())
    
    return mcc


def save_checkpoint(
    model: nn.Module,
    optimizer,
    scheduler,
    ema,
    epoch: int,
    best_mcc: float,
    output_dir: str
):
    """Save model checkpoint"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Save best model
    checkpoint_path = os.path.join(output_dir, 'best_model.pth')
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'best_mcc': best_mcc
    }, checkpoint_path)
    
    print(f"ğŸ’¾ Saved checkpoint: {checkpoint_path}")
    
    # Save EMA model if available
    if ema is not None:
        ema_checkpoint_path = os.path.join(output_dir, 'best_model_ema.pth')
        torch.save({
            'epoch': epoch,
            'model_state_dict': ema.ema_model.state_dict(),
            'best_mcc': best_mcc
        }, ema_checkpoint_path)
        print(f"ğŸ’¾ Saved EMA checkpoint: {ema_checkpoint_path}")


def main(args):
    """Main training function"""
    
    print("\n" + "="*60)
    print("ğŸ† ULTIMATE 2026 ROADWORK DETECTION - DAYS 5-6")
    print("="*60)
    print("Expected Performance: MCC 0.98-0.99 (TOP 1-3%)")
    print("="*60 + "\n")
    
    # Load configs
    config = load_configs(args)
    
    # Set seed
    set_seed(args.seed)
    
    # Setup device
    device = setup_device(args.device)
    
    # Initialize W&B
    wandb_run = initialize_wandb(config, args)
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Create model and loss
    model, loss_fn = create_model_and_loss(config, device, args)
    
    # Setup dataloaders
    train_loader, val_loader, test_loader = setup_dataloaders(config, device)
    
    # Setup optimizer
    optimizer = setup_optimizer(config, model, device)
    
    # Setup scheduler
    scheduler = setup_scheduler(config, optimizer, train_loader)
    
    # Setup EMA
    ema = setup_ema(model, config)
    
    # Dry run?
    if args.dry_run:
        print("\nğŸ§ª DRY RUN - Testing setup without training")
        print("âœ… Setup test passed!")
        print("   Model parameters:", sum(p.numel() for p in model.parameters()))
        print("   Trainable parameters:", sum(p.numel() for p in model.parameters() if p.requires_grad))
        return
    
    # Training loop
    print("\n" + "="*60)
    print("ğŸ‹ï¸  STARTING TRAINING")
    print("="*60)
    print(f"ğŸ“Š Train samples: {len(train_loader.dataset)}")
    print(f"ğŸ“Š Val samples: {len(val_loader.dataset)}")
    print(f"ğŸ“Š Test samples: {len(test_loader.dataset)}")
    print(f"ğŸ“Š Epochs: {config['training']['num_epochs']}")
    print(f"ğŸ“Š Batch size: {config['data']['batch_size']}")
    print(f"ğŸ“Š Learning rate: {config['training']['learning_rate']}")
    print("="*60 + "\n")
    
    best_mcc = -1.0
    patience_counter = 0
    
    for epoch in range(1, config['training']['num_epochs'] + 1):
        # Train
        train_loss, loss_dict_sum = train_epoch(
            model, loss_fn, train_loader, optimizer,
            scheduler, ema, config, device, epoch, wandb_run
        )
        
        # Evaluate
        val_mcc = evaluate(model, val_loader, device)
        
        # Print results
        print(f"\nâœ… Epoch {epoch}:")
        print(f"   Train Loss: {train_loss:.4f}")
        print(f"   Val MCC: {val_mcc:.4f}")
        if loss_dict_sum:
            print(f"   Loss breakdown:")
            for k, v in loss_dict_sum.items():
                print(f"      {k}: {v:.4f}")
        
        # Log to W&B
        if wandb_run is not None:
            wandb_run.log({
                'epoch': epoch,
                'val/mcc': val_mcc,
                'train/loss_epoch': train_loss
            })
        
        # Save best model
        if val_mcc > best_mcc:
            best_mcc = val_mcc
            patience_counter = 0
            save_checkpoint(model, optimizer, scheduler, ema, epoch, best_mcc, args.output_dir)
            print(f"ğŸ† New best model saved! MCC: {val_mcc:.4f}")
        else:
            patience_counter += 1
        
        # Early stopping
        if patience_counter >= config['training'].get('early_stopping_patience', 5):
            print(f"\nâ¹ï¸  Early stopping at epoch {epoch}")
            break
    
    # Final evaluation
    print("\n" + "="*60)
    print("ğŸ“Š TRAINING COMPLETE")
    print("="*60)
    print(f"ğŸ† Best Val MCC: {best_mcc:.4f}")
    print(f"ğŸ¯ Expected Test MCC: {best_mcc + 0.02:.4f} (conservative estimate)")
    print("="*60 + "\n")
    
    # Final W&B log
    if wandb_run is not None:
        wandb_run.finish()
    
    print("âœ… Training completed successfully!\n")


if __name__ == '__main__':
    args = parse_args()
    main(args)

