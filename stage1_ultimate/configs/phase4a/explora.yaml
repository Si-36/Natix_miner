# Phase 4a: ExPLoRA Domain Adaptation Configuration (2025 Best Practices)
# ==============================================================================
# 
# ExPLoRA = Extended Pretraining with LoRA for Domain Adaptation
# - Unsupervised SimCLR contrastive learning
# - Standard LoRA (NOT DoRA) for speed (8× faster)
# - Unfreeze last block + all LayerNorms
# - Strong augmentations (color jitter, blur, grayscale)
#
# Expected Gain: +6-8% MCC on downstream task
# Training Time: ~6 hours (2 GPUs, 30 epochs)

# Model Configuration
model:
  backbone_id: "facebook/dinov3-vith16plus-pretrain-lvd1689m"  # DINOv3 ViT-H/16+
  hidden_dim: 1536  # DINOv3 ViT-H hidden dimension

# Training Configuration
training:
  num_epochs: 30  # Domain adaptation is FAST (vs 150 for task fine-tuning)
  lr: 1e-4  # Conservative LR for LoRA (sensitive to learning rate)
  weight_decay: 0.05
  batch_size: 32  # Per-GPU micro-batch
  gradient_accumulation_steps: 8  # Effective batch: 32×8×2 GPUs×2 views = 1024
  warmup_epochs: 3  # 10% of 30 epochs

# SimCLR Contrastive Learning
simclr:
  temperature: 0.1  # Standard SimCLR temperature (Chen et al., 2020)
  projection_dim: 128  # Output embedding dimension
  hidden_dim: 2048  # Projection head hidden dimension
  dropout: 0.1  # Projection head dropout
  use_memory_bank: false  # In-batch negatives only (DDP all-gather)

# ExPLoRA LoRA Configuration (Standard LoRA, NOT DoRA)
explora:
  r: 32  # LoRA rank (higher for domain shift vs 16 for task)
  lora_alpha: 64  # 2× rank (standard scaling)
  target_modules:  # Only Q and V (not K or O) - ExPLoRA paper
    - "q_proj"
    - "v_proj"
  lora_dropout: 0.05
  use_dora: false  # CRITICAL: Standard LoRA for speed (DoRA is 8× slower)
  unfrozen_blocks: [23]  # Last block for ViT-H/16 (24 blocks, 0-indexed)

# Strong Augmentation for SimCLR
augmentation:
  crop_scale: [0.2, 1.0]  # Aggressive crop (vs [0.08, 1.0] for classification)
  color_jitter_strength: 0.8  # ±80% brightness/contrast/saturation
  gaussian_blur: true
  blur_kernel_size: 23
  blur_sigma: [0.1, 2.0]
  grayscale_prob: 0.2  # 20% chance → grayscale

# Hardware Configuration
hardware:
  num_gpus: 2  # DDP for multi-GPU scaling
  mixed_precision: true  # BF16 for A100/H100
  compile: false  # Skip compile for SimCLR (not well-optimized yet)

# Checkpointing
checkpoint:
  save_every_n_epochs: 10
  merge_lora_before_save: true  # Merge LoRA weights into backbone (no inference overhead)

