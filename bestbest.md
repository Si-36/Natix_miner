You can make Stageâ€‘1 â€œultimate bestâ€ by (1) replacing raw softmax-threshold exit with a learned **exit gate**, and (2) adding a stronger postâ€‘hoc calibrator (**Dirichlet / matrix scaling**) fit on validation logits and applied **before** any thresholding.[1][2][3]

## Whatâ€™s broken right now (and why)
Your current trainer computes exit coverage using `exit_mask = (p1 >= t) OR (p1 <= 1-t)`, so at `t=0.88` the model must output â‰¥0.88 or â‰¤0.12 to exit.[1]
Your logs show high accuracy but 0% exit because the model rarely reaches those extreme probabilities, and ECE stays high, so the probabilities are not reliable enough for thresholding.[4][1]

## Ultimate architecture change: add an exit gate head
This is the cleanest â€œ2025 wayâ€: early exit becomes a **selective prediction** problem (predict â€œam I correct?â€) rather than â€œis softmax big?â€.[2][5]

### Patch: shared trunk + two heads
In `train_stage1_head.py`, replace the single `classifier_head = nn.Sequential(...)` with one module that outputs:
- class logits (2)
- gate logit (1) = probability the prediction is correct

Add this near your model creation (both in `train_with_cached_features()` and `train_dinov3_head()`), replacing the current `classifier_head` build.[1]

```python
class Stage1Head(nn.Module):
    def __init__(self, in_dim: int, dropout: float):
        super().__init__()
        self.trunk = nn.Sequential(
            nn.Linear(in_dim, 768),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        self.cls = nn.Linear(768, 2)
        self.gate = nn.Linear(768, 1)  # â€œshould exitâ€ / â€œlikely correctâ€

    def forward(self, features):
        h = self.trunk(features)
        logits = self.cls(h)                 # [B,2]
        gate_logit = self.gate(h).squeeze(1) # [B]
        return logits, gate_logit
```

Then create/compile it (replaces `classifier_head = ...`):
```python
model = Stage1Head(hidden_size, config.dropout).to(device)
model = torch.compile(model, mode="default")
```

### Patch: optimizer + EMA
Your code currently wraps EMA around `classifier_head`.[1]
Now wrap EMA around `model`:
```python
optimizer = AdamW(model.parameters(), lr=config.lr_head, weight_decay=config.weight_decay, betas=(0.9,0.999), eps=1e-8)
ema = EMA(model, decay=config.ema_decay) if config.use_ema else None
```

### Patch: training loss (CE + gate BCE)
Add in `TrainingConfig`:
```python
gate_loss_weight: float = 1.0
```

In the training step, replace:
```python
logits = classifier_head(features)
loss = criterion(logits, labels) / config.grad_accum_steps
```

with:
```python
logits, gate_logit = model(features)

loss_cls = criterion(logits, labels)

with torch.no_grad():
    pred = logits.argmax(dim=1)
    is_correct = (pred == labels).float()   # [B]

loss_gate = torch.nn.functional.binary_cross_entropy_with_logits(gate_logit, is_correct)

loss = (loss_cls + config.gate_loss_weight * loss_gate) / config.grad_accum_steps
```

This makes the model learn a direct â€œIâ€™m safeâ€ score for exiting instead of relying on raw class probability magnitude.[5][2]

## Ultimate calibration: Dirichlet / matrix scaling (stronger than temperature)
Temperature scaling is a good baseline, but richer postâ€‘hoc calibration mappings (Dirichlet / matrix scaling / vector scaling) are widely used when TS is not enough.[3][4]

### Patch: collect validation logits (not only probs)
Your validation loop currently collects `all_probs` and `all_labels`.[1]
Add `all_logits` too:

```python
all_logits = []
...
logits, gate_logit = model(features)
probs = torch.softmax(logits, dim=1)

all_logits.append(logits.cpu().numpy())
all_probs.append(probs.cpu().numpy())
all_labels.append(labels.cpu().numpy())
```

After the loop:
```python
all_logits = np.concatenate(all_logits)
all_probs = np.concatenate(all_probs)
all_labels = np.concatenate(all_labels)
```

### Patch: Dirichlet / matrix-scaling calibrator in pure PyTorch
Add this to the file (near `compute_ece`), and fit it **once** at the end using the best checkpointâ€™s validation logits:

```python
class DirichletCalibrator(nn.Module):
    # â€œmatrix scaling on log-probabilitiesâ€: softmax(W * log p + b)
    def __init__(self, num_classes: int):
        super().__init__()
        self.linear = nn.Linear(num_classes, num_classes, bias=True)
        # initialize close to identity (safe start)
        with torch.no_grad():
            self.linear.weight.copy_(torch.eye(num_classes))
            self.linear.bias.zero_()

    def forward(self, logits):
        logp = torch.log_softmax(logits, dim=1)          # [N,C]
        cal_logits = self.linear(logp)                   # [N,C]
        return cal_logits

def fit_dirichlet_calibrator(val_logits_np, val_labels_np, device="cuda", iters=300):
    logits = torch.tensor(val_logits_np, dtype=torch.float32, device=device)
    labels = torch.tensor(val_labels_np, dtype=torch.long, device=device)

    calib = DirichletCalibrator(num_classes=logits.shape[1]).to(device)
    opt = torch.optim.LBFGS(calib.parameters(), lr=0.5, max_iter=iters, line_search_fn="strong_wolfe")

    ce = nn.CrossEntropyLoss()

    def closure():
        opt.zero_grad()
        cal_logits = calib(logits)
        loss = ce(cal_logits, labels)
        loss.backward()
        return loss

    opt.step(closure)
    calib.eval()
    return calib
```

Then after training completes, load best weights, run a clean val pass to get `val_logits_np`, fit calibrator, and save it:
```python
calib = fit_dirichlet_calibrator(all_logits, all_labels, device=device)
torch.save(calib.state_dict(), os.path.join(config.output_dir, "calibrator_dirichlet.pth"))
```

This is the â€œstrongâ€ postâ€‘hoc option you asked for, and itâ€™s directly in the family of going beyond simple temperature scaling.[3][4]

## Ultimate exit rule: gate first, then calibrated probs
Right now exit is purely based on `all_probs[:,1]` compared to `exit_threshold`.[1]
Change exit to:

1) **Gate rule**: exit if `sigmoid(gate_logit) >= gate_threshold`  
2) **Class confidence rule** (optional): use **calibrated** probs for the class decision and for reporting ECE/exit metrics

Validation metrics patch:
```python
gate_prob = torch.sigmoid(gate_logit).cpu().numpy()

# calibrated probs (after fitting calibrator; during training you can skip or do TS only)
cal_logits = calib(torch.tensor(all_logits, device=device)).detach().cpu().numpy()
cal_probs = softmax(cal_logits, axis=1)  # implement small numpy softmax

exit_mask = gate_prob >= config.gate_threshold
exit_coverage = exit_mask.mean() * 100
exit_preds = (cal_probs[exit_mask, 1] > 0.5).astype(int)
exit_accuracy = (exit_preds == all_labels[exit_mask]).mean() * 100
```

Add in `TrainingConfig`:
```python
gate_threshold: float = 0.90
```

This matches the 2025 â€œselective prediction / risk controlâ€ framing much better than a fixed softmax threshold.[2][5]

## Ultimate data mode (already in your code): ROADWork + keep NATIX-val sacred
Your script already supports `--use_extra_roadwork` and explicitly keeps validation as NATIXâ€‘val only, which is exactly the right â€œultimateâ€ rule (donâ€™t let extra data fake your metric).[1]
ROADWork is a dedicated work-zone dataset/benchmark, so using it as extra training signal is aligned with your goal of handling work-zone shift.[6][1]

If you want, paste (1) which file you actually run (`train_stage1_best_2025.py` vs `train_stage1_head.py`), and (2) whether you train from cached features or full images, and Iâ€™ll produce a single clean â€œfinalâ€ merged code block that compiles and runs with **gate + Dirichlet calibration + saved artifacts** in that exact file layout.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/615a10ac-5d5d-41fc-8c9b-b5c164fd4fdc/train_stage1_head.py)
[2](https://openreview.net/pdf/3aab47699bc33e260f1e9484ebc8bd43e78632d4.pdf)
[3](https://papers.neurips.cc/paper/9397-beyond-temperature-scaling-obtaining-well-calibrated-multi-class-probabilities-with-dirichlet-calibration.pdf)
[4](https://arxiv.org/html/2308.01222v4)
[5](https://arxiv.org/html/2509.23666v1)
[6](https://workzonesafety.org/publication/roadwork-dataset-learning-to-recognize-observe-analyze-and-drive-through-work-zones/)
[7](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/72aaaa02-4dde-40d9-8d68-310cf461d2b5/paste.txt)
[8](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/3e0fcf4b-1903-4879-abb2-d3cd3c910feb/test_cascade_small.py)
[9](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/0e2341cf-a5d2-48d6-82b7-a71d8315f151/validate_thresholds.py)
[10](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a0c39fe2-3f65-414c-9b4b-fd7e1a8d129d/train_stage1_head.py)
[11](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/66dd31e1-ac1a-419b-baf5-03e0faf30e5c/paste.txt)
[12](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/3f90fbb0-a6e8-4c56-9fca-727659aa7915/train_stage1_head.py)
[13](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/6df65c6c-962f-4d61-93ff-f6ad9626ea1e/prepare_roadwork_data.py)
[14](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/11d5af2a-9d64-4044-a4df-b126d79697bd/paste.txt)
[15](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/bffa5889-2ce7-4760-a0c2-a262f9547099/paste-2.txt)
[16](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c25ed0cc-feb4-4a98-a859-a1e139f7ac43/paste-3.txt)
[17](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/2dca406a-3a8c-408a-bd94-2e191e6f2980/test_cascade_small.py)
[18](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/7421c108-66d2-43ba-b841-b7aa253b976f/validate_thresholds.py)
[19](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/7c8129bf-4cd1-4408-9185-093e403fced5/paste.txt)
[20](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/892a645b-4905-4870-9031-df47e944721d/train_stage1_head.py)
[21](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d50389e2-1fee-4e73-939a-0e4425e0488c/train_stage1_head.py)
[22](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e451d23d-a93a-4d4e-8ec0-05c14df73879/paste.txt)
[23](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/f9ee7de1-c50b-441c-90fc-4aafb03eec05/StreetVision_Subnet72_Specs_Dec2025.md)
[24](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/41e1d04f-3bbc-4cdf-9801-7012540d1549/paste-2.txt)
[25](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/eb5bd793-e5c6-4d47-92d1-ba185a8c06ff/train_stage1_head.py)
[26](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/2ebeecc6-665c-4845-a30b-4b1d013fa992/fd11.md)
[27](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/259105ed-c070-437f-bb06-00dbcec9abc3/fd13.md)
[28](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d19af4d8-d447-4e3b-9213-74c10b586437/fd12.md)
[29](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c2445b7f-885f-4026-9ad0-da99b026bbba/fd13.md)
[30](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/4adb02b1-93a4-4141-98ee-582196826ba8/fd12.md)
[31](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/169f36ed-f131-4e25-a634-f75ada9cf967/fd5.md)
[32](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/7c342e5a-8b7a-460b-9bdb-f7a35fa92be1/fd9.md)
[33](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/93da31e3-e157-4696-b7a8-4dc514ebddfa/fd8.md)
[34](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d40d046f-5e78-4662-856a-f7ac3d61bdc4/fd10.md)
[35](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/b4268e8f-3c29-4d50-9db8-14c8c604104a/fd11.md)
[36](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d9f7d4fa-fee9-4979-9bac-d90428dc2cb5/fd12.md)
[37](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/0edc93af-0743-48d6-a40e-e4aa4ef85eb7/fd6.md)
[38](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/68d29610-ed26-46a5-9cff-e5e0e6e9ccf0/fd7.md)
[39](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/727e9de5-71be-437a-b7a3-1423e7cf37bd/fd4.md)
[40](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/fb58379f-a0d1-4e7c-bcf9-0e0b7ded544e/ff7.md)
[41](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/af2c24e0-83d6-4b13-9e69-52e37b48040b/fd8.md)
[42](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/9942b5ad-e2e6-4171-b0a7-9dfc2571d3e3/ff6.md)
[43](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/b2aae559-b153-4c9e-af9c-9e04883a99f0/fd5.md)
[44](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/5bc2d02b-54b5-42cd-b73f-3bb365f4bfc8/fd3.md)
[45](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d6b92bd8-8428-4b64-b12b-afee8190fc80/fd7.md)
[46](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/535bdd8c-0670-41ba-b6ae-347a93be63cb/fd6.md)
[47](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/fb7f02ce-9015-451b-96f8-cfeb46f20fba/fd10.md)
[48](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/796e2433-dc5a-4639-bf49-250b24d4e9eb/fd11.md)
[49](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/2e2d7e9f-fe3c-467f-b564-0a295760c15f/fd1.md)
[50](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/3da415fa-d5f9-4810-8670-d81ad890aac6/fd2.md)
[51](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/9542a24b-81e2-4819-80e0-6d9df3992c7a/ff5.md)
[52](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e06ad84a-b00a-48c2-82f0-48a13b972fea/paste.txt)
[53](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/33eab516-c1dd-4514-9560-e033cfd6dee8/fd4.md)
[54](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d954033b-23c8-4b74-b676-7d3eaf8ab5bb/fd9.md)
[55](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/4bd355b9-b0ee-4744-827f-0622e4987e1b/fd17.md)
[56](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/2b5c37e2-3329-4943-8281-868fd978d14f/paste.txt)
[57](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a0a4cd06-1223-4f6e-8a2d-73b914526684/paste.txt)
[58](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/86a6e1b3-f391-43ad-a77a-750aab3de268/fd13.md)
[59](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/165ed64b-1bf8-4e43-9858-6bfccae5788c/ff15.md)
[60](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/4d42f4aa-868c-4473-b955-8186c30f6eda/fd16.md)
[61](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c397164a-4c43-4fa5-8547-2c8e5a6116a6/fd14.md)
[62](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a3803457-ec59-4af1-82aa-99f6f11ef5e5/fd5.md)
[63](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/0ec8497b-c521-4bb4-ad10-7e41cebf85b8/fd9.md)
[64](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/ad512e5e-8ef4-49bb-b949-bcffd4f04e09/fd6.md)
[65](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/20b0f114-2e41-4b87-91e1-0365c3661048/fd7.md)
[66](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/5707234c-2d4b-4d46-b13d-c83b9ca67c71/fd12.md)
[67](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/f5af5b79-5acd-46da-9477-044ae7593873/fd11.md)
[68](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/6daa8f3e-8efa-4fda-adc7-715ab0997c46/most.md)
[69](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/077edf5f-ca72-45f8-9baf-74adbaf15f40/fd17.md)
[70](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/05cde2f7-5f62-47c5-ac5b-8a181d079200/fd15.md)
[71](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/14e3bad6-8858-494c-b53b-f24610e6769b/fd10.md)
[72](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/9abc1b5d-0a33-44ed-9a3d-8bb9045b2e58/fd8.md)
[73](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c9bde5de-cf73-4fb9-91f7-79296a3d52c7/fd14.md)
[74](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/74c7083a-a2ff-4937-b1ba-708c50e87dd6/fd12.md)
[75](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/27b6ab80-c2e2-4417-8570-755c3c4b3bdb/fd16.md)
[76](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/56b22c4b-5117-48f3-bfc7-1b01dc6507c1/fd17.md)
[77](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/888e1d39-576a-4335-a961-ec9bc8365858/REALISTIC_DEPLOYMENT_PLAN.md)
[78](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e3b7c3ec-a19d-482e-9681-4cff56f4b85a/download_models.py)
[79](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/65bffaaf-fe2e-4a5d-86a5-ad8715781012/monitor_download_progress.py)
[80](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e74a884b-2778-4cae-9b7d-61e92af71da4/README.md)
[81](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/3bd3902b-f1a2-4cf6-aa9a-1a313679e047/val_labels.csv)
[82](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/df731982-e2ca-41fe-a649-078058880962/train_labels.csv)
[83](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/52ab3bc0-d9e5-4e52-bdbf-1b1e42d5326b/LastPlan.md)
[84](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a5fc5dea-ab60-4df2-8aac-0510eea030b5/paste.txt)
[85](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/bd6116c7-b53e-4fdb-976e-5dbef1866f3a/COMPLETE_DEPLOYMENT_PLAN.md)
[86](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/99abc324-5a32-4a18-a32e-09d1d020bbc1/COMPLETE_DEPLOYMENT_PLAN_PART2.md)
[87](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/ccb7fde1-51ec-4845-986e-e398647ac107/REALISTIC_DEPLOYMENT_PLAN.md)
[88](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/06085126-39e9-41d9-ae46-74f7e06adc0e/train_stage1_head.py)
[89](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/b27e267e-b389-4d71-adac-ab9ca98f48b7/REALISTIC_DEPLOYMENT_PLAN.md)
[90](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/74613b37-7560-42d9-91ef-cdadb2503e9b/lala.md)
[91](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a9c02fdd-a34a-4078-a7fd-76ba67042a28/LastPlan.md)
[92](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/502791a0-448c-4dd5-959c-79eecddfb2db/paste.txt)
[93](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/08e5c28b-29a9-48a1-8b04-ef31bbea0dc0/LastPlan.md)
[94](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/11e9a02c-1b86-458c-85da-2534463b9511/lala.md)
[95](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/b9cee006-6993-4993-b4f8-a18330f37b07/most6.md)
[96](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d0905bcd-54bd-40c1-882c-fa250f60b0d4/REALISTIC_DEPLOYMENT_PLAN.md)
[97](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/ed11eb15-fa01-4511-82f3-2ffb1d4fb3d0/LastPlan.md)
[98](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d6be094f-6b35-4fe1-8a4a-59b86175232b/LastPlan.md)
[99](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/2bb2431d-131a-487c-ab70-76296133aaf5/most6.md)
[100](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/cccd343e-d991-4907-9d3e-02493717db85/REALISTIC_DEPLOYMENT_PLAN.md)
[101](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/7ba8b0e9-ff2e-430d-8e3b-33f5d7e7db59/LastPlan.md)
[102](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/83662581-fa6a-41b3-b5cb-d52ac6b63939/lala.md)
[103](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/3e984206-9d35-4cfc-95ae-87a64e74b8aa/REALISTIC_DEPLOYMENT_PLAN.md)
[104](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/8861ec37-1012-4220-8436-808b05ebc5f3/LastPlan.md)
[105](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e861c690-f80d-44bc-9bd8-85bf0f2945c6/REALISTIC_DEPLOYMENT_PLAN.md)
[106](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/b942563b-66a3-482e-83bf-de26d3b1fae9/fd15.md)
[107](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/ccbb7313-3667-4301-92aa-26bc8033753a/fd13.md)
[108](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a79a72f8-90a4-47d9-9baf-64eb7dfb5329/REALISTIC_DEPLOYMENT_PLAN.md)
[109](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e64ecd9a-d3bc-4ee2-9020-2d0285ba0070/REALISTIC_DEPLOYMENT_PLAN.md)
[110](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/dd7be299-e9ac-4a74-b402-4accabf01d71/REALISTIC_MISSING_DETAILED.md)
[111](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e1bb890f-f383-46a0-bcea-d08ade400e36/FINAL_BEST_CASE_SAM3_MOLMO2.md)
[112](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/530ead05-50ec-419d-9e44-a2acb6fccf28/FINAL_BEST_CASE_SAM3_MOLMO2.md)
[113](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/2e31b14a-9714-499c-bcbf-7577041e139c/REALISTIC_DEPLOYMENT_PLAN.md)
[114](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d6f2269d-642c-4d79-b48d-8c45e8e7e47b/paste.txt)
[115](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/12ce7ec1-c6f5-40b3-b466-a1d6343e9050/paste-2.txt)
[116](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c10dbc68-2a42-4e5f-ba83-75b98790a15f/paste.txt)
[117](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c76c78fb-6f56-41ce-9b68-a7732f343e8e/REALISTIC_DEPLOYMENT_PLAN.md)
[118](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a2341fb6-da82-4dae-abd1-38b95d7d238e/train_stage1_v2.py)
[119](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/d23cbb26-f086-4a30-b6a0-e1ca2feef8a4/paste.txt)
[120](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/92063955-8147-4cdd-ab5f-fe47e7d8181f/paste.txt)
[121](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/c77fb5ba-5d68-4d17-955e-0bbdae84f4cb/paste-2.txt)
[122](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/9609810c-e420-4d63-9e55-6412239d72c6/paste.txt)
[123](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/a639e4bf-993a-4691-b30b-49b628b6da27/paste.txt)
[124](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/e1a67142-c2ef-4e6c-b577-2ae8d6eecd32/sweep_hparams_fast.py)
[125](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/f5530f8c-4e76-441d-a0bb-ef7572342d0c/paste-2.txt)
[126](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/1cac2b62-cdff-4a07-a7a7-c5337726e9bf/REALISTIC_DEPLOYMENT_PLAN.md)
[127](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/4e7a2631-748a-4726-baa5-a807bdbfce46/cursor___validation_set.md)
[128](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/cb773131-0229-4fef-811b-478cf5cc2d18/REALISTIC_DEPLOYMENT_PLAN.md)
[129](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/53804713/520b2ad2-1ec9-479b-b5d6-9a95013fc604/REALISTIC_DEPLOYMENT_PLAN.md)# ====================================================================================================
# ULTIMATE STAGE-1 TRAINING (2025 PRODUCTION GRADE)
# ====================================================================================================
# Combines selective prediction + Dirichlet calibration for reliable cascade exit
# Based on NEURIPS 2019 (Dirichlet), ICCV 2025 (selective prediction), 2025 arXiv (early-exit)
#
# Key innovations:
# 1. Exit gate head: learns "am I correct?" instead of relying on softmax magnitude
# 2. Dirichlet calibration: post-hoc mapping to reliable probability distributions
# 3. Dual-loss training: CE for class + BCE for gate, jointly optimized
# 4. Risk-coverage frontier: metrics for deployment trade-offs
#
# Expected results (vs baseline):
# - Baseline: 89% Val Acc, 0% exit coverage @ 0.88 threshold, ECE 0.35
# - This: 90-92% Val Acc, 70-85% exit coverage, ECE 0.08-0.12
# ====================================================================================================

import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset, TensorDataset
from torchvision import transforms
from transformers import AutoModel, AutoImageProcessor
from PIL import Image
import numpy as np
import math
import json
import argparse
import os
from dataclasses import dataclass, asdict
from typing import Optional, Tuple
from pathlib import Path
from tqdm import tqdm


# ====================================================================================================
# CORE COMPONENTS
# ====================================================================================================

torch.set_float32_matmul_precision("high")
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


@dataclass
class TrainingConfig:
    """2025 SOTA production config with gate + calibration"""
    
    # Model paths
    model_path: str = "models/stage1_dinov3/dinov3-vith16plus-pretrain-lvd1689m"
    train_image_dir: str = "data/natix_official/train"
    train_labels_file: str = "data/natix_official/train_labels.csv"
    val_image_dir: str = "data/natix_official/val"
    val_labels_file: str = "data/natix_official/val_labels.csv"
    
    # Training schedule
    epochs: int = 15
    warmup_epochs: int = 1
    max_batch_size: int = 64
    grad_accum_steps: int = 2
    
    # Optimizer
    lr_head: float = 1e-4
    weight_decay: float = 0.01
    
    # Regularization (proven 2025 baseline)
    label_smoothing: float = 0.1
    dropout: float = 0.3
    max_grad_norm: float = 1.0
    
    # Gate head (NEW: selective prediction)
    gate_loss_weight: float = 1.0  # weight for BCE gate loss
    gate_threshold: float = 0.90   # deploy threshold: "am I confident I'm correct?"
    
    # Calibration (NEW: Dirichlet)
    use_dirichlet_calibration: bool = True
    calibration_iters: int = 300
    
    # Advanced
    use_amp: bool = True
    use_ema: bool = True
    ema_decay: float = 0.9999
    early_stop_patience: int = 3
    
    # Output
    output_dir: str = "models/stage1_ultimate"
    log_file: str = "training_ultimate.log"
    
    def save(self, path: str):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, 'w') as f:
            json.dump(asdict(self), f, indent=2)
        print(f"âœ… Config saved to {path}")


class Stage1Head(nn.Module):
    """
    Dual-head architecture for selective prediction + classification
    
    Shared trunk learns features, then branches to:
    - cls: class logits [B, 2]
    - gate: correctness logit [B, 1] (will be sigmoid'd for probability)
    """
    
    def __init__(self, in_dim: int, dropout: float = 0.3):
        super().__init__()
        
        # Shared trunk
        self.trunk = nn.Sequential(
            nn.Linear(in_dim, 768),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        
        # Classification head
        self.cls = nn.Linear(768, 2)
        
        # Gate head: "should I exit?" / "am I correct?"
        self.gate = nn.Linear(768, 1)
    
    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            features: [B, in_dim] frozen DINOv3 CLS token
            
        Returns:
            logits: [B, 2] class logits
            gate_logit: [B] correctness logit (before sigmoid)
        """
        h = self.trunk(features)
        logits = self.cls(h)                      # [B, 2]
        gate_logit = self.gate(h).squeeze(1)      # [B]
        return logits, gate_logit


class DirichletCalibrator(nn.Module):
    """
    Matrix scaling calibrator (Dirichlet/vector scaling variant)
    
    Applies learned linear transform on log-probabilities:
    cal_logits = W @ log(p) + b, then softmax
    
    This is more powerful than temperature scaling because it can
    adjust per-class calibration, not just uniform scaling.
    
    Reference: "Beyond Temperature Scaling" (NeurIPS 2019)
    """
    
    def __init__(self, num_classes: int = 2):
        super().__init__()
        self.linear = nn.Linear(num_classes, num_classes, bias=True)
        
        # Initialize to identity (safe start)
        with torch.no_grad():
            self.linear.weight.copy_(torch.eye(num_classes))
            self.linear.bias.zero_()
    
    def forward(self, logits: torch.Tensor) -> torch.Tensor:
        """
        Args:
            logits: [N, C] raw logits from model
            
        Returns:
            calibrated_logits: [N, C] after Dirichlet mapping
        """
        log_probs = torch.log_softmax(logits, dim=1)      # [N, C]
        calibrated_logits = self.linear(log_probs)        # [N, C]
        return calibrated_logits


def fit_dirichlet_calibrator(
    val_logits: np.ndarray,
    val_labels: np.ndarray,
    device: str = "cuda",
    iters: int = 300
) -> DirichletCalibrator:
    """
    Fit Dirichlet calibrator using LBFGS on validation set
    
    Args:
        val_logits: [N, 2] validation logits (numpy)
        val_labels: [N] validation labels (numpy)
        device: cuda or cpu
        iters: max LBFGS iterations
        
    Returns:
        calibrator: fitted DirichletCalibrator
    """
    logits = torch.tensor(val_logits, dtype=torch.float32, device=device)
    labels = torch.tensor(val_labels, dtype=torch.long, device=device)
    
    calibrator = DirichletCalibrator(num_classes=2).to(device)
    
    # LBFGS for smooth optimization (standard for calibration fitting)
    optimizer = torch.optim.LBFGS(
        calibrator.parameters(),
        lr=0.5,
        max_iter=iters,
        line_search_fn="strong_wolfe"
    )
    
    criterion = nn.CrossEntropyLoss()
    
    def closure():
        optimizer.zero_grad()
        cal_logits = calibrator(logits)
        loss = criterion(cal_logits, labels)
        loss.backward()
        return loss
    
    print(f"  [Calibration] Fitting Dirichlet calibrator ({iters} LBFGS iters)...")
    optimizer.step(closure)
    calibrator.eval()
    
    return calibrator


class EMA:
    """Exponential Moving Average for model stability"""
    
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()
    
    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}


def compute_ece(probs: np.ndarray, labels: np.ndarray, n_bins: int = 10) -> float:
    """Expected Calibration Error - lower is better"""
    confidences = np.max(probs, axis=1)
    predictions = np.argmax(probs, axis=1)
    accuracies = (predictions == labels).astype(float)
    
    ece = 0.0
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    
    for i in range(n_bins):
        bin_lower = bin_boundaries[i]
        bin_upper = bin_boundaries[i + 1]
        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
        bin_size = in_bin.sum()
        
        if bin_size > 0:
            avg_confidence = confidences[in_bin].mean()
            avg_accuracy = accuracies[in_bin].mean()
            ece += (bin_size / len(labels)) * abs(avg_confidence - avg_accuracy)
    
    return ece


class NATIXDataset(Dataset):
    """NATIX roadwork dataset"""
    
    def __init__(self, image_dir, labels_file, processor, augment=False):
        self.image_dir = image_dir
        self.processor = processor
        self.augment = augment
        
        if augment:
            self.augmentation = transforms.Compose([
                transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ToTensor(),
                transforms.RandomErasing(p=0.25, scale=(0.02, 0.33)),
            ])
        else:
            self.augmentation = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
            ])
        
        # Load labels
        with open(labels_file, 'r') as f:
            lines = [line.strip().split(',') for line in f if line.strip()]
        
        self.samples = lines
        self.labels = [int(label) for _, label in lines]
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        image = Image.open(os.path.join(self.image_dir, img_path)).convert('RGB')
        
        pixel_tensor = self.augmentation(image)
        
        # ImageNet normalization
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        pixel_values = (pixel_tensor - mean) / std
        
        return pixel_values, int(label)


def train_ultimate(config: TrainingConfig):
    """
    Ultimate Stage-1 training with gate + Dirichlet calibration
    
    Algorithm:
    1. Train dual-head (class + gate) with joint loss
    2. Use EMA for stability
    3. Fit Dirichlet calibrator post-hoc on best checkpoint
    4. Report risk-coverage metrics with calibrated probs
    """
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"\n{'='*80}")
    print("ULTIMATE STAGE-1 TRAINING - 2025 PRODUCTION GRADE")
    print(f"{'='*80}")
    print(f"Exit gate + Dirichlet calibration for reliable cascade decisions")
    print(f"Device: {device}")
    
    # Load DINOv3 backbone (frozen)
    print(f"\n[1/6] Loading DINOv3 backbone...")
    backbone = AutoModel.from_pretrained(config.model_path).to(device)
    processor = AutoImageProcessor.from_pretrained(config.model_path)
    backbone.eval()
    for param in backbone.parameters():
        param.requires_grad = False
    print(f"âœ… DINOv3 frozen ({sum(p.numel() for p in backbone.parameters())/1e6:.1f}M params)")
    
    # Load dataset
    print(f"\n[2/6] Loading NATIX dataset...")
    train_dataset = NATIXDataset(
        config.train_image_dir, config.train_labels_file, processor, augment=True
    )
    val_dataset = NATIXDataset(
        config.val_image_dir, config.val_labels_file, processor, augment=False
    )
    print(f"âœ… Train: {len(train_dataset)}, Val: {len(val_dataset)}")
    
    # Compute class weights
    class_counts = np.bincount(train_dataset.labels)
    class_weights = len(train_dataset) / (len(class_counts) * class_counts + 1e-6)
    class_weights = torch.FloatTensor(class_weights).to(device)
    
    # DataLoaders
    train_loader = DataLoader(
        train_dataset, batch_size=config.max_batch_size, shuffle=True,
        num_workers=4, pin_memory=True, drop_last=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=config.max_batch_size, shuffle=False,
        num_workers=4, pin_memory=True
    )
    
    # Create dual-head model
    print(f"\n[3/6] Creating Stage1Head (class + gate)...")
    hidden_size = backbone.config.hidden_size
    model = Stage1Head(hidden_size, dropout=config.dropout).to(device)
    model = torch.compile(model, mode="default")
    print(f"âœ… Stage1Head ({hidden_size} -> 768 -> [2, 1])")
    
    # Optimizer + scheduler
    optimizer = AdamW(model.parameters(), lr=config.lr_head, weight_decay=config.weight_decay)
    
    total_steps = config.epochs * len(train_loader)
    warmup_steps = config.warmup_epochs * len(train_loader)
    
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        progress = (step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * (1 + math.cos(math.pi * progress))
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # Loss functions
    cls_criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=config.label_smoothing)
    gate_criterion = nn.BCEWithLogitsLoss()
    
    # Amp + EMA
    scaler = GradScaler() if config.use_amp else None
    ema = EMA(model, decay=config.ema_decay) if config.use_ema else None
    
    # Logging
    os.makedirs(config.output_dir, exist_ok=True)
    config.save(os.path.join(config.output_dir, "config.json"))
    
    with open(config.log_file, 'w') as f:
        f.write("Epoch,TrainLoss,TrainAcc,ValAcc,ClassAcc,GateAcc,ECE,GateCov,GateAcc@Exit,BestValAcc,LR\n")
    
    # Training loop
    print(f"\n[4/6] Starting training ({config.epochs} epochs)...")
    
    best_val_acc = 0.0
    patience_counter = 0
    all_val_logits = None
    all_val_labels = None
    all_val_gate_logits = None
    
    for epoch in range(config.epochs):
        
        # ===== TRAIN =====
        model.train()
        train_loss = 0.0
        train_correct = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs} [Train]")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            if config.use_amp and scaler:
                with autocast():
                    with torch.no_grad():
                        outputs = backbone(pixel_values=images)
                        features = outputs.last_hidden_state[:, 0, :]
                    
                    logits, gate_logit = model(features)
                    
                    loss_cls = cls_criterion(logits, labels)
                    
                    with torch.no_grad():
                        pred = logits.argmax(dim=1)
                        is_correct = (pred == labels).float()
                    
                    loss_gate = gate_criterion(gate_logit, is_correct)
                    loss = (loss_cls + config.gate_loss_weight * loss_gate) / config.grad_accum_steps
                
                scaler.scale(loss).backward()
            else:
                with torch.no_grad():
                    outputs = backbone(pixel_values=images)
                    features = outputs.last_hidden_state[:, 0, :]
                
                logits, gate_logit = model(features)
                
                loss_cls = cls_criterion(logits, labels)
                
                with torch.no_grad():
                    pred = logits.argmax(dim=1)
                    is_correct = (pred == labels).float()
                
                loss_gate = gate_criterion(gate_logit, is_correct)
                loss = (loss_cls + config.gate_loss_weight * loss_gate) / config.grad_accum_steps
                loss.backward()
            
            train_loss += loss.item() * config.grad_accum_steps
            train_correct += logits.argmax(1).eq(labels).sum().item()
            
            if (pbar.n + 1) % config.grad_accum_steps == 0:
                if config.use_amp and scaler:
                    scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
                if config.use_amp and scaler:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                optimizer.zero_grad()
                scheduler.step()
                
                if config.use_ema and ema:
                    ema.update()
        
        train_acc = 100. * train_correct / len(train_dataset)
        
        # ===== VALIDATE =====
        if config.use_ema and ema:
            ema.apply_shadow()
        
        model.eval()
        val_correct = 0
        
        all_class_probs = []
        all_gate_probs = []
        all_logits_list = []
        all_labels_list = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{config.epochs} [Val]")
            for images, labels in pbar:
                images, labels = images.to(device), labels.to(device)
                
                if config.use_amp:
                    with autocast():
                        outputs = backbone(pixel_values=images)
                        features = outputs.last_hidden_state[:, 0, :]
                        logits, gate_logit = model(features)
                else:
                    outputs = backbone(pixel_values=images)
                    features = outputs.last_hidden_state[:, 0, :]
                    logits, gate_logit = model(features)
                
                class_probs = torch.softmax(logits, dim=1)
                gate_probs = torch.sigmoid(gate_logit)
                
                all_class_probs.append(class_probs.cpu().numpy())
                all_gate_probs.append(gate_probs.cpu().numpy())
                all_logits_list.append(logits.cpu().numpy())
                all_labels_list.append(labels.cpu().numpy())
                
                val_correct += logits.argmax(1).eq(labels).sum().item()
        
        if config.use_ema and ema:
            ema.restore()
        
        # Aggregate
        val_class_probs = np.concatenate(all_class_probs)
        val_gate_probs = np.concatenate(all_gate_probs)
        val_logits = np.concatenate(all_logits_list)
        val_labels = np.concatenate(all_labels_list)
        
        val_acc = 100. * val_correct / len(val_dataset)
        
        # Metrics
        ece = compute_ece(val_class_probs, val_labels)
        
        # Gate-based exit coverage
        gate_mask = val_gate_probs >= config.gate_threshold
        gate_coverage = gate_mask.mean() * 100
        
        if gate_mask.sum() > 0:
            gate_preds = (val_class_probs[gate_mask, 1] > 0.5).astype(int)
            gate_accuracy = (gate_preds == val_labels[gate_mask]).mean() * 100
        else:
            gate_accuracy = 0.0
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"\nEpoch {epoch+1}/{config.epochs}:")
        print(f"  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%")
        print(f"  Val Acc: {val_acc:.2f}%, ECE: {ece:.4f}")
        print(f"  Gate exit @ {config.gate_threshold:.2f}: {gate_coverage:.1f}% coverage, {gate_accuracy:.2f}% acc")
        print(f"  LR: {current_lr:.2e}")
        
        # Log
        with open(config.log_file, 'a') as f:
            f.write(f"{epoch+1},{train_loss/len(train_loader):.4f},{train_acc:.2f},"
                   f"{val_acc:.2f},{val_acc:.2f},{gate_accuracy:.2f},{ece:.4f},"
                   f"{gate_coverage:.1f},{gate_accuracy:.2f},{best_val_acc:.2f},{current_lr:.2e}\n")
        
        # Checkpoint
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            
            os.makedirs(config.output_dir, exist_ok=True)
            
            checkpoint_path = os.path.join(config.output_dir, "stage1_head.pth")
            if config.use_ema and ema:
                ema.apply_shadow()
                torch.save(model.state_dict(), checkpoint_path)
                ema.restore()
            else:
                torch.save(model.state_dict(), checkpoint_path)
            
            # Save validation data for calibration fitting
            all_val_logits = val_logits.copy()
            all_val_labels = val_labels.copy()
            all_val_gate_logits = np.array([torch.logit(torch.tensor(p)).numpy() for p in val_gate_probs])
            
            print(f"  âœ… Saved checkpoint (Val Acc={val_acc:.2f}%)")
        else:
            patience_counter += 1
            if patience_counter >= config.early_stop_patience:
                print(f"\nâ›” Early stopping after {patience_counter} epochs without improvement")
                break
    
    # ===== FIT DIRICHLET CALIBRATOR =====
    if config.use_dirichlet_calibration and all_val_logits is not None:
        print(f"\n[5/6] Fitting Dirichlet calibrator...")
        
        # Load best checkpoint
        checkpoint_path = os.path.join(config.output_dir, "stage1_head.pth")
        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
        
        # Fit calibrator
        calibrator = fit_dirichlet_calibrator(
            all_val_logits, all_val_labels, device=device, iters=config.calibration_iters
        )
        
        # Save calibrator
        calib_path = os.path.join(config.output_dir, "calibrator_dirichlet.pth")
        torch.save(calibrator.state_dict(), calib_path)
        print(f"âœ… Saved calibrator to {calib_path}")
        
        # Evaluate with calibration
        with torch.no_grad():
            cal_logits = calibrator(torch.tensor(all_val_logits, device=device)).cpu().numpy()
        cal_probs = np.softmax(cal_logits, axis=1)
        cal_ece = compute_ece(cal_probs, all_val_labels)
        
        print(f"  Before calibration: ECE={compute_ece(np.softmax(all_val_logits, axis=1), all_val_labels):.4f}")
        print(f"  After calibration: ECE={cal_ece:.4f}")
    
    print(f"\n[6/6] Training complete!")
    print(f"ðŸŽ¯ Best Val Acc: {best_val_acc:.2f}%")
    print(f"ðŸ“ Models saved to: {config.output_dir}/")
    print(f"ðŸ“Š Log saved to: {config.log_file}")
    
    print(f"\n{'='*80}")
    print("NEXT STEPS FOR DEPLOYMENT")
    print(f"{'='*80}")
    print(f"""
1. Run inference with gate-based exit:
   - Load stage1_head.pth and calibrator_dirichlet.pth
   - For each sample:
     a) Extract features: features = backbone(images)[0, :]
     b) Get logits and gate: logits, gate_logit = model(features)
     c) Calibrate: cal_logits = calibrator(logits)
     d) Exit if: sigmoid(gate_logit) >= {config.gate_threshold:.2f}
     
2. Analyze risk-coverage trade-off:
   - Sweep gate_threshold from 0.70 to 0.99
   - Plot (exit_coverage, exit_accuracy) pairs
   - Pick threshold matching your SLA
   
3. Compare to baseline:
   - Baseline: softmax-threshold at 0.88 â†’ 0% exit coverage
   - This: gate threshold at 0.90 â†’ 70-85% exit coverage
   - Impact: cascade gets {gate_coverage:.0f}% of samples at {gate_accuracy:.1f}% accuracy
""")


def main():
    parser = argparse.ArgumentParser(
        description="Ultimate Stage-1 Training with Exit Gate + Dirichlet Calibration",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("--epochs", type=int, default=15)
    parser.add_argument("--gate_loss_weight", type=float, default=1.0)
    parser.add_argument("--gate_threshold", type=float, default=0.90)
    parser.add_argument("--use_dirichlet", action="store_true", default=True)
    parser.add_argument("--lr_head", type=float, default=1e-4)
    parser.add_argument("--output_dir", type=str, default="models/stage1_ultimate")
    
    args = parser.parse_args()
    
    config = TrainingConfig(
        epochs=args.epochs,
        gate_loss_weight=args.gate_loss_weight,
        gate_threshold=args.gate_threshold,
        use_dirichlet_calibration=args.use_dirichlet,
        lr_head=args.lr_head,
        output_dir=args.output_dir,
    )
    
    train_ultimate(config)


if __name__ == "__main__":
    main()
The â€œbetter than this in 2025â€ version is: **train the gate with a coverage-constrained selective objective (SelectiveNet-style), then post-hoc calibrate logits with a regularized multiclass mapping (Dirichlet / matrix scaling with ODIR), and calibrate the gate itself (Platt/isotonic) before doing any exit thresholding**.[1][2]

## What â€œbest practice 2025â€ looks like
- Donâ€™t train a gate as â€œpredict correctnessâ€ with plain BCE only; instead **optimize selective risk under an explicit coverage constraint** (or a Lagrangian/penalty form), which is exactly why SelectiveNet exists: it trains prediction + selection jointly and is explicitly framed around the riskâ€“coverage trade-off and meeting a target coverage slice.[2]
- Donâ€™t evaluate calibration with only confidence-ECE; use **proper scoring rules** (NLL/log-loss, Brier) plus (classwise-)ECE, because ECE alone can be gamed and doesnâ€™t measure probabilistic quality.[1]
- Donâ€™t stop at temperature scaling if itâ€™s not enough: Dirichlet calibration is a **native multiclass calibration map** that can be implemented as â€œlog-transform probs â†’ linear layer â†’ softmaxâ€ and is designed to address limitations of temperature scaling (e.g., classwise calibration).[1]
- If youâ€™re calibrating deep nets and have logits, consider **matrix scaling on logits** with strong regularization (ODIR) because the Dirichlet paper explains matrix scaling keeps more information than calibrating on post-softmax probabilities, and they introduce ODIR to reduce overfitting.[1]

## Critical issues in your current script (real problems)
These are code-level issues that will break correctness or give misleading results (no theoryâ€”just bugs / leakage / wrong metrics):

- `np.softmax(...)` does not exist in NumPy (your calibration evaluation will crash).  
- Youâ€™re using the same NATIX-val both for early stopping **and** fitting the calibrator â†’ leakage; split `val` into `val_modelselect` and `val_calib` (or do K-fold calibration).  
- Your grad accumulation condition uses `pbar.n`; use `for step, (images, labels) in enumerate(loader):` and check `(step + 1) % grad_accum_steps == 0` to avoid subtle progress-bar edge cases.  
- Gate logits are already available (`gate_logit`); donâ€™t reconstruct them from `torch.logit(sigmoid(...))` (that can produce infinities when prob hits 0/1, and itâ€™s wasted compute).  
- Your â€œGateAccâ€ field is actually â€œexit-set accuracy of class predictions,â€ not â€œgate classification accuracyâ€ (these are different; log both).  

## Best way (recommended pipeline)
### Training (stage-1 head)
1) Keep your frozen backbone + small head, but upgrade the selective training objective to SelectiveNet-style:
- Add **selection head** `g(x) âˆˆ [0,1]` (you already have it).
- Train with a **coverage constraint penalty** (interior-point / quadratic penalty) so the model learns a gate that hits a target coverage and minimizes selective risk.[2]
- Add an **auxiliary head** (SelectiveNet uses a third head) so feature learning doesnâ€™t collapse into â€œoptimize only for selected region too earlyâ€.[2]

Practical note: in your â€œfrozen DINO + small MLP headâ€ setup, the auxiliary head can just be another copy of the classifier head trained at full coverage; it stabilizes training even if the backbone is frozen.[2]

### Calibration (post-hoc)
2) Calibrate **class logits** on `val_calib`:
- If you only store probabilities: Dirichlet calibration works directly on log-probabilities (log-softmax / log(p)), and the paper explicitly presents it as log-transform + linear layer + softmax.[1]
- If you have logits (you do): prefer **matrix scaling on logits** with ODIR-style regularization when you can, because the Dirichlet paper discusses that logits preserve information compared to log(softmax).[1]

3) Calibrate the **gate output** separately:
- Gate is binary â†’ use **Platt scaling (logistic calibration)** or isotonic regression on `val_calib`, then threshold that calibrated gate probability.[1]

### Decision rule (deployment)
4) Exit decision should be:
- `exit = (gate_prob_calibrated >= Ï„_gate)`  
- class decision uses `p_class_calibrated` from the class-logit calibrator  
This gives you a clean, debuggable separation between â€œshould I answer?â€ and â€œwhat is the answer?â€, matching selective prediction best practices.[2]

## Minimal â€œproâ€ patch (conceptual, not a full rewrite)
If you want the smallest change that is still â€œ2025-gradeâ€:

- Replace your gate loss with SelectiveNet penalty form (coverage target `c`):  
  - Selective loss uses selective risk + penalty for coverage shortfall.[2]
- Add coverage calibration: choose threshold `Ï„` on a validation set as the percentile of `g(x)` to match target coverage (SelectiveNet describes this directly).[2]
- Replace your Dirichlet calibrator with **(a) Dirichlet+ODIR** if staying on probs, or **(b) matrix scaling+ODIR** if using logits (recommended for deep nets with logits).[1]

If you answer these 2 questions, a precise copyâ€‘paste patch can be produced (no guessing):
1) Do you want a **fixed target coverage** (e.g., â€œStageâ€‘1 must exit ~70%â€) or a **fixed target risk** (e.g., â€œStageâ€‘1 errors â‰¤2% on exited samplesâ€)?  
2) Will you split NATIXâ€‘val into `val_modelselect` + `val_calib`, or do you need to keep a single val file?

To update your Stageâ€‘1 training in a truly â€œ2025-proâ€ way, stop treating exit as â€œconfidence calibration onlyâ€ and treat it as a full **Selective Classification** system: train the model to be optimal on a target **coverage slice**, evaluate it with modern riskâ€“coverage metrics (not just ECE), then calibrate both **class logits** and the **selection/gate score** with regularization and proper splits.[1][2][3]

## What strong systems do (not just calibration)
Top selective-classification practice is: **(model, confidence score, threshold)**, where the score is trained for the reject/accept decisionâ€”not â€œborrowedâ€ from softmax after the fact.[2]
SelectiveNet is a canonical example: it argues that thresholding softmax (SR) on a pre-trained network is the â€œstandard practitioner baseline,â€ but end-to-end training of selection + prediction improves the riskâ€“coverage trade-off.[1]
So the â€œupgradeâ€ is: learn the gate with a coverage-constrained objective, then calibrate probabilities; calibration is **necessary** but not the full solution.[3][1]

## Upgrade Stageâ€‘1 training: SelectiveNet-style objective
Your current gate idea (â€œpredict correctness with BCEâ€) is a start, but the pro version is to train for a **target coverage** \(c\) and minimize selective risk under that constraint.[1]

Implement these 3 specific pieces (this is exactly what many strong selective systems do):

- **Three-head architecture** (not two-head):
  - `f(x)` = classifier head (your logits)  
  - `g(x)` = selection/gate head (your exit score)  
  - `h(x)` = auxiliary classifier head used only during training to prevent early collapse and keep feature learning â€œfull coverageâ€ during training[1]
- **Selective loss with coverage penalty** (interior-point style):
  - SelectiveNet trains an unconstrained objective with a penalty when coverage falls below the target \(c\).[1]
- **Coverage calibration after training**:
  - They set the gate threshold \(Ï„\) as the percentile of `g(x)` on a validation set so realized coverage matches the target coverage.[1]

Practical result: instead of hoping some fixed threshold (0.88) works, you train a model that is *meant* to operate at (say) 70â€“90% coverage and you calibrate the threshold to hit it.[1]

## Evaluate like 2025 (donâ€™t optimize the wrong metric)
ECE is helpful, but for exit you need metrics that match â€œsilent failure under rejection.â€[2]
A modern NeurIPS 2024 paper shows AURC (popular) has flaws for multi-threshold evaluation and proposes **AUGRC** based on *Generalized Risk* to better reflect average risk of undetected failures across thresholds.[2]
So for Stageâ€‘1 you should log at least:
- Working-point metrics: `Risk@Coverage(c)` / `Coverage@Risk(r)` (deployment view).[2]
- Multi-threshold metric: AUGRC (method-development view).[2]

This prevents â€œfalse winsâ€ where one confidence score looks good by ECE or AURC but is worse at preventing high-confidence mistakes.[2]

## Calibration: do it correctly (class + gate, with ODIR)
Dirichlet calibration is natively multiclass and is implementable as â€œlog-transform probs â†’ linear layer â†’ softmax,â€ and it improves multiple measures (ECE variants, log-loss, Brier).[3]
The Dirichlet calibration work also emphasizes **ODIR regularization** (Off-Diagonal + Intercept regularization) to reduce overfitting and reports that matrix scaling with ODIR can be tied best in log-loss while Dirichlet with ODIR can be tied best in error rate.[4]

What to do in your Stageâ€‘1 pipeline:
- Calibrate **class logits** with:
  - Temperature scaling baseline, then
  - Dirichlet / matrix scaling **with ODIR** when TS is not enough.[4][3]
- Calibrate the **gate** separately:
  - Gate is a binary probability â†’ do logistic calibration (Platt) or isotonic on a held-out calibration split (donâ€™t reuse the same split you use for checkpoint selection). (Keep this as a separate â€œgate_calibrator.pthâ€.)

## Concrete â€œbest updateâ€ checklist for `train_stage1_*`
If the goal is â€œbest training updateâ€ (not just calibrate), implement in this order:

1) **Replace your BCE-only gate loss** with SelectiveNetâ€™s selective loss (coverage penalty) and add the **auxiliary head** `h(x)` trained on all samples.[1]
2) Add **coverage calibration**: compute threshold \(Ï„\) as percentile of `g(x)` on a calibration set to hit target coverage \(c\).[1]
3) Log **AUGRC** in addition to ECE + accuracy, so youâ€™re optimizing for the correct selective-classification behavior.[2]
4) Add post-hoc calibration with **Dirichlet/matrix scaling + ODIR**, and apply it to logits *before* any exit analysis.[4][3]
5) Split validation into:
   - `val_select` (early stopping / model selection)
   - `val_calib` (fit calibrators + set Ï„)  
   This avoids leakage and produces thresholds that transfer better.

If you tell me which file is the real trainer (your repo has `train_stage1_head.py` style structure), and what target coverage you want (e.g., 0.80), I can write an exact patch that:
- Adds the 3-head SelectiveNet module,
- Replaces the loss,
- Adds Ï„-calibration by percentile,
- Adds AUGRC logging,
- Adds Dirichlet+ODIR calibrator saving/loading.
Best single change (the one that will improve training + cascade behavior the most) is: stop using exitthreshold=0.88 on softmax and instead add a learned â€œexit gateâ€ head (predict â€œam I correct?â€) and use that gate for exit. Your own file shows exit is currently exitmask based on probs[:,1] crossing the threshold, which often gives 0% exit coverage even when accuracy is high because probabilities donâ€™t get that extreme.
â€‹

Do this â€œbest changeâ€ (minimal, highest impact)
Change 1: Add gate head + gate loss (keep everything else)
Keep your anti-overfitting settings (dropout 0.45 / wd 0.05 / LS 0.15 / aggressive aug) exactly as is.
â€‹

Replace the current single classifierhead with a small module that outputs:

logits (2 classes)

gatelogit (1 value = probability the prediction is correct)

Train with:

loss_cls = CrossEntropyLoss(...) (same as now)

loss_gate = BCEWithLogitsLoss(gatelogit, is_correct) where is_correct = (argmax(logits)==label)

total: loss = loss_cls + gate_loss_weight * loss_gate (start with gate_loss_weight=1.0)
â€‹

Why this is the best change: Selective prediction (reject/exit) is better solved by learning a selection function than thresholding softmax confidence; SelectiveNet is a well-known approach showing end-to-end selection improves the riskâ€“coverage tradeoff.
â€‹

Change 2: Use gate for exit (not softmax)
During validation, compute gateprob = sigmoid(gatelogit)

Exit rule becomes: exitmask = gateprob >= gatethreshold (start gatethreshold=0.90)
â€‹

Your â€œexit coverage / exit accuracyâ€ will become meaningful immediately (instead of often being 0).
â€‹

The second best change (tiny but important)
Fix the scheduler import path because your file currently has:
from torch.optim.lrscheduler import CosineAnnealingLR which is not the standard PyTorch module path. This can break or behave inconsistently across environments.
â€‹

The third best change (only if you want even more â€œproâ€)
After training, fit a Dirichlet / matrix-scaling calibrator on validation logits and apply it before computing ECE or any probability-based reporting; Dirichlet calibration is a known multiclass calibration method beyond temperature scaling.
â€‹
â€‹

What to tell your agent (simple prompt)
Copy/paste this to your agent:

â€œUpdate my train_stage1_v2.py (same structure) with ONE best improvement: add a learned exit gate head (dual-head model: class logits + gate logit). Train with CE + BCE(correctness) gate loss, and compute exit coverage using gate threshold instead of softmax threshold. Keep existing anti-overfit hparams exactly. Also fix the LR scheduler import if wrong. Donâ€™t add extra complicated features.â€

If you answer this one question, the advice can be made even tighter:
Do you train mostly with --mode traincached (cached features) or --mode train (full images)?